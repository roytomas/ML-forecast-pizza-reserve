{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict(X,w,b):return X * w + b\n",
    "def loss(X,Y,w,b):return np.average((predict(X,w,b) - Y) ** 2)\n",
    "def train(X,Y,iterations,lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        current_loss = loss(X,Y,w,b)\n",
    "        print(\"Iteration %4d => Loss: %.6f\"% (i,current_loss))\n",
    "        if loss(X,Y,w + lr,b) < current_loss:\n",
    "            w += lr\n",
    "        elif loss(X,Y,w - lr,b) < current_loss:\n",
    "            w -= lr\n",
    "        elif loss(X,Y,w,b + lr) < current_loss:\n",
    "            b += lr\n",
    "        elif loss(X,Y,w,b - lr) < current_loss:\n",
    "            b -= lr\n",
    "        else:\n",
    "            return w,b\n",
    "    raise Exception(\"Couldn't converge within %d iterations\" % iterations)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 1139.800000\n",
      "Iteration    1 => Loss: 1130.089340\n",
      "Iteration    2 => Loss: 1120.421360\n",
      "Iteration    3 => Loss: 1110.796060\n",
      "Iteration    4 => Loss: 1101.213440\n",
      "Iteration    5 => Loss: 1091.673500\n",
      "Iteration    6 => Loss: 1082.176240\n",
      "Iteration    7 => Loss: 1072.721660\n",
      "Iteration    8 => Loss: 1063.309760\n",
      "Iteration    9 => Loss: 1053.940540\n",
      "Iteration   10 => Loss: 1044.614000\n",
      "Iteration   11 => Loss: 1035.330140\n",
      "Iteration   12 => Loss: 1026.088960\n",
      "Iteration   13 => Loss: 1016.890460\n",
      "Iteration   14 => Loss: 1007.734640\n",
      "Iteration   15 => Loss: 998.621500\n",
      "Iteration   16 => Loss: 989.551040\n",
      "Iteration   17 => Loss: 980.523260\n",
      "Iteration   18 => Loss: 971.538160\n",
      "Iteration   19 => Loss: 962.595740\n",
      "Iteration   20 => Loss: 953.696000\n",
      "Iteration   21 => Loss: 944.838940\n",
      "Iteration   22 => Loss: 936.024560\n",
      "Iteration   23 => Loss: 927.252860\n",
      "Iteration   24 => Loss: 918.523840\n",
      "Iteration   25 => Loss: 909.837500\n",
      "Iteration   26 => Loss: 901.193840\n",
      "Iteration   27 => Loss: 892.592860\n",
      "Iteration   28 => Loss: 884.034560\n",
      "Iteration   29 => Loss: 875.518940\n",
      "Iteration   30 => Loss: 867.046000\n",
      "Iteration   31 => Loss: 858.615740\n",
      "Iteration   32 => Loss: 850.228160\n",
      "Iteration   33 => Loss: 841.883260\n",
      "Iteration   34 => Loss: 833.581040\n",
      "Iteration   35 => Loss: 825.321500\n",
      "Iteration   36 => Loss: 817.104640\n",
      "Iteration   37 => Loss: 808.930460\n",
      "Iteration   38 => Loss: 800.798960\n",
      "Iteration   39 => Loss: 792.710140\n",
      "Iteration   40 => Loss: 784.664000\n",
      "Iteration   41 => Loss: 776.660540\n",
      "Iteration   42 => Loss: 768.699760\n",
      "Iteration   43 => Loss: 760.781660\n",
      "Iteration   44 => Loss: 752.906240\n",
      "Iteration   45 => Loss: 745.073500\n",
      "Iteration   46 => Loss: 737.283440\n",
      "Iteration   47 => Loss: 729.536060\n",
      "Iteration   48 => Loss: 721.831360\n",
      "Iteration   49 => Loss: 714.169340\n",
      "Iteration   50 => Loss: 706.550000\n",
      "Iteration   51 => Loss: 698.973340\n",
      "Iteration   52 => Loss: 691.439360\n",
      "Iteration   53 => Loss: 683.948060\n",
      "Iteration   54 => Loss: 676.499440\n",
      "Iteration   55 => Loss: 669.093500\n",
      "Iteration   56 => Loss: 661.730240\n",
      "Iteration   57 => Loss: 654.409660\n",
      "Iteration   58 => Loss: 647.131760\n",
      "Iteration   59 => Loss: 639.896540\n",
      "Iteration   60 => Loss: 632.704000\n",
      "Iteration   61 => Loss: 625.554140\n",
      "Iteration   62 => Loss: 618.446960\n",
      "Iteration   63 => Loss: 611.382460\n",
      "Iteration   64 => Loss: 604.360640\n",
      "Iteration   65 => Loss: 597.381500\n",
      "Iteration   66 => Loss: 590.445040\n",
      "Iteration   67 => Loss: 583.551260\n",
      "Iteration   68 => Loss: 576.700160\n",
      "Iteration   69 => Loss: 569.891740\n",
      "Iteration   70 => Loss: 563.126000\n",
      "Iteration   71 => Loss: 556.402940\n",
      "Iteration   72 => Loss: 549.722560\n",
      "Iteration   73 => Loss: 543.084860\n",
      "Iteration   74 => Loss: 536.489840\n",
      "Iteration   75 => Loss: 529.937500\n",
      "Iteration   76 => Loss: 523.427840\n",
      "Iteration   77 => Loss: 516.960860\n",
      "Iteration   78 => Loss: 510.536560\n",
      "Iteration   79 => Loss: 504.154940\n",
      "Iteration   80 => Loss: 497.816000\n",
      "Iteration   81 => Loss: 491.519740\n",
      "Iteration   82 => Loss: 485.266160\n",
      "Iteration   83 => Loss: 479.055260\n",
      "Iteration   84 => Loss: 472.887040\n",
      "Iteration   85 => Loss: 466.761500\n",
      "Iteration   86 => Loss: 460.678640\n",
      "Iteration   87 => Loss: 454.638460\n",
      "Iteration   88 => Loss: 448.640960\n",
      "Iteration   89 => Loss: 442.686140\n",
      "Iteration   90 => Loss: 436.774000\n",
      "Iteration   91 => Loss: 430.904540\n",
      "Iteration   92 => Loss: 425.077760\n",
      "Iteration   93 => Loss: 419.293660\n",
      "Iteration   94 => Loss: 413.552240\n",
      "Iteration   95 => Loss: 407.853500\n",
      "Iteration   96 => Loss: 402.197440\n",
      "Iteration   97 => Loss: 396.584060\n",
      "Iteration   98 => Loss: 391.013360\n",
      "Iteration   99 => Loss: 385.485340\n",
      "Iteration  100 => Loss: 380.000000\n",
      "Iteration  101 => Loss: 374.557340\n",
      "Iteration  102 => Loss: 369.157360\n",
      "Iteration  103 => Loss: 363.800060\n",
      "Iteration  104 => Loss: 358.485440\n",
      "Iteration  105 => Loss: 353.213500\n",
      "Iteration  106 => Loss: 347.984240\n",
      "Iteration  107 => Loss: 342.797660\n",
      "Iteration  108 => Loss: 337.653760\n",
      "Iteration  109 => Loss: 332.552540\n",
      "Iteration  110 => Loss: 327.494000\n",
      "Iteration  111 => Loss: 322.478140\n",
      "Iteration  112 => Loss: 317.504960\n",
      "Iteration  113 => Loss: 312.574460\n",
      "Iteration  114 => Loss: 307.686640\n",
      "Iteration  115 => Loss: 302.841500\n",
      "Iteration  116 => Loss: 298.039040\n",
      "Iteration  117 => Loss: 293.279260\n",
      "Iteration  118 => Loss: 288.562160\n",
      "Iteration  119 => Loss: 283.887740\n",
      "Iteration  120 => Loss: 279.256000\n",
      "Iteration  121 => Loss: 274.666940\n",
      "Iteration  122 => Loss: 270.120560\n",
      "Iteration  123 => Loss: 265.616860\n",
      "Iteration  124 => Loss: 261.155840\n",
      "Iteration  125 => Loss: 256.737500\n",
      "Iteration  126 => Loss: 252.361840\n",
      "Iteration  127 => Loss: 248.028860\n",
      "Iteration  128 => Loss: 243.738560\n",
      "Iteration  129 => Loss: 239.490940\n",
      "Iteration  130 => Loss: 235.286000\n",
      "Iteration  131 => Loss: 231.123740\n",
      "Iteration  132 => Loss: 227.004160\n",
      "Iteration  133 => Loss: 222.927260\n",
      "Iteration  134 => Loss: 218.893040\n",
      "Iteration  135 => Loss: 214.901500\n",
      "Iteration  136 => Loss: 210.952640\n",
      "Iteration  137 => Loss: 207.046460\n",
      "Iteration  138 => Loss: 203.182960\n",
      "Iteration  139 => Loss: 199.362140\n",
      "Iteration  140 => Loss: 195.584000\n",
      "Iteration  141 => Loss: 191.848540\n",
      "Iteration  142 => Loss: 188.155760\n",
      "Iteration  143 => Loss: 184.505660\n",
      "Iteration  144 => Loss: 180.898240\n",
      "Iteration  145 => Loss: 177.333500\n",
      "Iteration  146 => Loss: 173.811440\n",
      "Iteration  147 => Loss: 170.332060\n",
      "Iteration  148 => Loss: 166.895360\n",
      "Iteration  149 => Loss: 163.501340\n",
      "Iteration  150 => Loss: 160.150000\n",
      "Iteration  151 => Loss: 156.841340\n",
      "Iteration  152 => Loss: 153.575360\n",
      "Iteration  153 => Loss: 150.352060\n",
      "Iteration  154 => Loss: 147.171440\n",
      "Iteration  155 => Loss: 144.033500\n",
      "Iteration  156 => Loss: 140.938240\n",
      "Iteration  157 => Loss: 137.885660\n",
      "Iteration  158 => Loss: 134.875760\n",
      "Iteration  159 => Loss: 131.908540\n",
      "Iteration  160 => Loss: 128.984000\n",
      "Iteration  161 => Loss: 126.102140\n",
      "Iteration  162 => Loss: 123.262960\n",
      "Iteration  163 => Loss: 120.466460\n",
      "Iteration  164 => Loss: 117.712640\n",
      "Iteration  165 => Loss: 115.001500\n",
      "Iteration  166 => Loss: 112.333040\n",
      "Iteration  167 => Loss: 109.707260\n",
      "Iteration  168 => Loss: 107.124160\n",
      "Iteration  169 => Loss: 104.583740\n",
      "Iteration  170 => Loss: 102.086000\n",
      "Iteration  171 => Loss: 99.630940\n",
      "Iteration  172 => Loss: 97.218560\n",
      "Iteration  173 => Loss: 94.848860\n",
      "Iteration  174 => Loss: 92.521840\n",
      "Iteration  175 => Loss: 90.237500\n",
      "Iteration  176 => Loss: 87.995840\n",
      "Iteration  177 => Loss: 85.796860\n",
      "Iteration  178 => Loss: 83.640560\n",
      "Iteration  179 => Loss: 81.526940\n",
      "Iteration  180 => Loss: 79.456000\n",
      "Iteration  181 => Loss: 77.427740\n",
      "Iteration  182 => Loss: 75.442160\n",
      "Iteration  183 => Loss: 73.499260\n",
      "Iteration  184 => Loss: 71.599040\n",
      "Iteration  185 => Loss: 69.741500\n",
      "Iteration  186 => Loss: 67.926640\n",
      "Iteration  187 => Loss: 66.154460\n",
      "Iteration  188 => Loss: 64.424960\n",
      "Iteration  189 => Loss: 62.738140\n",
      "Iteration  190 => Loss: 61.094000\n",
      "Iteration  191 => Loss: 59.492540\n",
      "Iteration  192 => Loss: 57.933760\n",
      "Iteration  193 => Loss: 56.417660\n",
      "Iteration  194 => Loss: 54.944240\n",
      "Iteration  195 => Loss: 53.513500\n",
      "Iteration  196 => Loss: 52.125440\n",
      "Iteration  197 => Loss: 50.780060\n",
      "Iteration  198 => Loss: 49.477360\n",
      "Iteration  199 => Loss: 48.217340\n",
      "Iteration  200 => Loss: 47.000000\n",
      "Iteration  201 => Loss: 45.825340\n",
      "Iteration  202 => Loss: 44.693360\n",
      "Iteration  203 => Loss: 43.604060\n",
      "Iteration  204 => Loss: 42.557440\n",
      "Iteration  205 => Loss: 41.553500\n",
      "Iteration  206 => Loss: 40.592240\n",
      "Iteration  207 => Loss: 39.673660\n",
      "Iteration  208 => Loss: 38.797760\n",
      "Iteration  209 => Loss: 37.964540\n",
      "Iteration  210 => Loss: 37.174000\n",
      "Iteration  211 => Loss: 36.426140\n",
      "Iteration  212 => Loss: 35.720960\n",
      "Iteration  213 => Loss: 35.058460\n",
      "Iteration  214 => Loss: 34.438640\n",
      "Iteration  215 => Loss: 33.861500\n",
      "Iteration  216 => Loss: 33.327040\n",
      "Iteration  217 => Loss: 32.835260\n",
      "Iteration  218 => Loss: 32.386160\n",
      "Iteration  219 => Loss: 31.979740\n",
      "Iteration  220 => Loss: 31.616000\n",
      "Iteration  221 => Loss: 31.294940\n",
      "Iteration  222 => Loss: 31.016560\n",
      "Iteration  223 => Loss: 30.780860\n",
      "Iteration  224 => Loss: 30.587840\n",
      "Iteration  225 => Loss: 30.437500\n",
      "Iteration  226 => Loss: 30.329840\n",
      "Iteration  227 => Loss: 30.264860\n",
      "Iteration  228 => Loss: 30.242560\n",
      "Iteration  229 => Loss: 30.199460\n",
      "Iteration  230 => Loss: 30.156560\n",
      "Iteration  231 => Loss: 30.113860\n",
      "Iteration  232 => Loss: 30.071360\n",
      "Iteration  233 => Loss: 30.029060\n",
      "Iteration  234 => Loss: 29.986960\n",
      "Iteration  235 => Loss: 29.945060\n",
      "Iteration  236 => Loss: 29.903360\n",
      "Iteration  237 => Loss: 29.861860\n",
      "Iteration  238 => Loss: 29.860760\n",
      "Iteration  239 => Loss: 29.816860\n",
      "Iteration  240 => Loss: 29.773160\n",
      "Iteration  241 => Loss: 29.729660\n",
      "Iteration  242 => Loss: 29.686360\n",
      "Iteration  243 => Loss: 29.643260\n",
      "Iteration  244 => Loss: 29.600360\n",
      "Iteration  245 => Loss: 29.557660\n",
      "Iteration  246 => Loss: 29.515160\n",
      "Iteration  247 => Loss: 29.472860\n",
      "Iteration  248 => Loss: 29.430760\n",
      "Iteration  249 => Loss: 29.388860\n",
      "Iteration  250 => Loss: 29.347160\n",
      "Iteration  251 => Loss: 29.305660\n",
      "Iteration  252 => Loss: 29.264360\n",
      "Iteration  253 => Loss: 29.223260\n",
      "Iteration  254 => Loss: 29.182360\n",
      "Iteration  255 => Loss: 29.182340\n",
      "Iteration  256 => Loss: 29.139040\n",
      "Iteration  257 => Loss: 29.095940\n",
      "Iteration  258 => Loss: 29.053040\n",
      "Iteration  259 => Loss: 29.010340\n",
      "Iteration  260 => Loss: 28.967840\n",
      "Iteration  261 => Loss: 28.925540\n",
      "Iteration  262 => Loss: 28.883440\n",
      "Iteration  263 => Loss: 28.841540\n",
      "Iteration  264 => Loss: 28.799840\n",
      "Iteration  265 => Loss: 28.758340\n",
      "Iteration  266 => Loss: 28.717040\n",
      "Iteration  267 => Loss: 28.675940\n",
      "Iteration  268 => Loss: 28.635040\n",
      "Iteration  269 => Loss: 28.594340\n",
      "Iteration  270 => Loss: 28.553840\n",
      "Iteration  271 => Loss: 28.513540\n",
      "Iteration  272 => Loss: 28.473440\n",
      "Iteration  273 => Loss: 28.471900\n",
      "Iteration  274 => Loss: 28.429400\n",
      "Iteration  275 => Loss: 28.387100\n",
      "Iteration  276 => Loss: 28.345000\n",
      "Iteration  277 => Loss: 28.303100\n",
      "Iteration  278 => Loss: 28.261400\n",
      "Iteration  279 => Loss: 28.219900\n",
      "Iteration  280 => Loss: 28.178600\n",
      "Iteration  281 => Loss: 28.137500\n",
      "Iteration  282 => Loss: 28.096600\n",
      "Iteration  283 => Loss: 28.055900\n",
      "Iteration  284 => Loss: 28.015400\n",
      "Iteration  285 => Loss: 27.975100\n",
      "Iteration  286 => Loss: 27.935000\n",
      "Iteration  287 => Loss: 27.895100\n",
      "Iteration  288 => Loss: 27.855400\n",
      "Iteration  289 => Loss: 27.815900\n",
      "Iteration  290 => Loss: 27.815440\n",
      "Iteration  291 => Loss: 27.773540\n",
      "Iteration  292 => Loss: 27.731840\n",
      "Iteration  293 => Loss: 27.690340\n",
      "Iteration  294 => Loss: 27.649040\n",
      "Iteration  295 => Loss: 27.607940\n",
      "Iteration  296 => Loss: 27.567040\n",
      "Iteration  297 => Loss: 27.526340\n",
      "Iteration  298 => Loss: 27.485840\n",
      "Iteration  299 => Loss: 27.445540\n",
      "Iteration  300 => Loss: 27.405440\n",
      "Iteration  301 => Loss: 27.365540\n",
      "Iteration  302 => Loss: 27.325840\n",
      "Iteration  303 => Loss: 27.286340\n",
      "Iteration  304 => Loss: 27.247040\n",
      "Iteration  305 => Loss: 27.207940\n",
      "Iteration  306 => Loss: 27.169040\n",
      "Iteration  307 => Loss: 27.130340\n",
      "Iteration  308 => Loss: 27.128360\n",
      "Iteration  309 => Loss: 27.087260\n",
      "Iteration  310 => Loss: 27.046360\n",
      "Iteration  311 => Loss: 27.005660\n",
      "Iteration  312 => Loss: 26.965160\n",
      "Iteration  313 => Loss: 26.924860\n",
      "Iteration  314 => Loss: 26.884760\n",
      "Iteration  315 => Loss: 26.844860\n",
      "Iteration  316 => Loss: 26.805160\n",
      "Iteration  317 => Loss: 26.765660\n",
      "Iteration  318 => Loss: 26.726360\n",
      "Iteration  319 => Loss: 26.687260\n",
      "Iteration  320 => Loss: 26.648360\n",
      "Iteration  321 => Loss: 26.609660\n",
      "Iteration  322 => Loss: 26.571160\n",
      "Iteration  323 => Loss: 26.532860\n",
      "Iteration  324 => Loss: 26.494760\n",
      "Iteration  325 => Loss: 26.493860\n",
      "Iteration  326 => Loss: 26.453360\n",
      "Iteration  327 => Loss: 26.413060\n",
      "Iteration  328 => Loss: 26.372960\n",
      "Iteration  329 => Loss: 26.333060\n",
      "Iteration  330 => Loss: 26.293360\n",
      "Iteration  331 => Loss: 26.253860\n",
      "Iteration  332 => Loss: 26.214560\n",
      "Iteration  333 => Loss: 26.175460\n",
      "Iteration  334 => Loss: 26.136560\n",
      "Iteration  335 => Loss: 26.097860\n",
      "Iteration  336 => Loss: 26.059360\n",
      "Iteration  337 => Loss: 26.021060\n",
      "Iteration  338 => Loss: 25.982960\n",
      "Iteration  339 => Loss: 25.945060\n",
      "Iteration  340 => Loss: 25.907360\n",
      "Iteration  341 => Loss: 25.869860\n",
      "Iteration  342 => Loss: 25.832560\n",
      "Iteration  343 => Loss: 25.830140\n",
      "Iteration  344 => Loss: 25.790440\n",
      "Iteration  345 => Loss: 25.750940\n",
      "Iteration  346 => Loss: 25.711640\n",
      "Iteration  347 => Loss: 25.672540\n",
      "Iteration  348 => Loss: 25.633640\n",
      "Iteration  349 => Loss: 25.594940\n",
      "Iteration  350 => Loss: 25.556440\n",
      "Iteration  351 => Loss: 25.518140\n",
      "Iteration  352 => Loss: 25.480040\n",
      "Iteration  353 => Loss: 25.442140\n",
      "Iteration  354 => Loss: 25.404440\n",
      "Iteration  355 => Loss: 25.366940\n",
      "Iteration  356 => Loss: 25.329640\n",
      "Iteration  357 => Loss: 25.292540\n",
      "Iteration  358 => Loss: 25.255640\n",
      "Iteration  359 => Loss: 25.218940\n",
      "Iteration  360 => Loss: 25.217600\n",
      "Iteration  361 => Loss: 25.178500\n",
      "Iteration  362 => Loss: 25.139600\n",
      "Iteration  363 => Loss: 25.100900\n",
      "Iteration  364 => Loss: 25.062400\n",
      "Iteration  365 => Loss: 25.024100\n",
      "Iteration  366 => Loss: 24.986000\n",
      "Iteration  367 => Loss: 24.948100\n",
      "Iteration  368 => Loss: 24.910400\n",
      "Iteration  369 => Loss: 24.872900\n",
      "Iteration  370 => Loss: 24.835600\n",
      "Iteration  371 => Loss: 24.798500\n",
      "Iteration  372 => Loss: 24.761600\n",
      "Iteration  373 => Loss: 24.724900\n",
      "Iteration  374 => Loss: 24.688400\n",
      "Iteration  375 => Loss: 24.652100\n",
      "Iteration  376 => Loss: 24.616000\n",
      "Iteration  377 => Loss: 24.615740\n",
      "Iteration  378 => Loss: 24.577240\n",
      "Iteration  379 => Loss: 24.538940\n",
      "Iteration  380 => Loss: 24.500840\n",
      "Iteration  381 => Loss: 24.462940\n",
      "Iteration  382 => Loss: 24.425240\n",
      "Iteration  383 => Loss: 24.387740\n",
      "Iteration  384 => Loss: 24.350440\n",
      "Iteration  385 => Loss: 24.313340\n",
      "Iteration  386 => Loss: 24.276440\n",
      "Iteration  387 => Loss: 24.239740\n",
      "Iteration  388 => Loss: 24.203240\n",
      "Iteration  389 => Loss: 24.166940\n",
      "Iteration  390 => Loss: 24.130840\n",
      "Iteration  391 => Loss: 24.094940\n",
      "Iteration  392 => Loss: 24.059240\n",
      "Iteration  393 => Loss: 24.023740\n",
      "Iteration  394 => Loss: 23.988440\n",
      "Iteration  395 => Loss: 23.986660\n",
      "Iteration  396 => Loss: 23.948960\n",
      "Iteration  397 => Loss: 23.911460\n",
      "Iteration  398 => Loss: 23.874160\n",
      "Iteration  399 => Loss: 23.837060\n",
      "Iteration  400 => Loss: 23.800160\n",
      "Iteration  401 => Loss: 23.763460\n",
      "Iteration  402 => Loss: 23.726960\n",
      "Iteration  403 => Loss: 23.690660\n",
      "Iteration  404 => Loss: 23.654560\n",
      "Iteration  405 => Loss: 23.618660\n",
      "Iteration  406 => Loss: 23.582960\n",
      "Iteration  407 => Loss: 23.547460\n",
      "Iteration  408 => Loss: 23.512160\n",
      "Iteration  409 => Loss: 23.477060\n",
      "Iteration  410 => Loss: 23.442160\n",
      "Iteration  411 => Loss: 23.407460\n",
      "Iteration  412 => Loss: 23.406760\n",
      "Iteration  413 => Loss: 23.369660\n",
      "Iteration  414 => Loss: 23.332760\n",
      "Iteration  415 => Loss: 23.296060\n",
      "Iteration  416 => Loss: 23.259560\n",
      "Iteration  417 => Loss: 23.223260\n",
      "Iteration  418 => Loss: 23.187160\n",
      "Iteration  419 => Loss: 23.151260\n",
      "Iteration  420 => Loss: 23.115560\n",
      "Iteration  421 => Loss: 23.080060\n",
      "Iteration  422 => Loss: 23.044760\n",
      "Iteration  423 => Loss: 23.009660\n",
      "Iteration  424 => Loss: 22.974760\n",
      "Iteration  425 => Loss: 22.940060\n",
      "Iteration  426 => Loss: 22.905560\n",
      "Iteration  427 => Loss: 22.871260\n",
      "Iteration  428 => Loss: 22.837160\n",
      "Iteration  429 => Loss: 22.803260\n",
      "Iteration  430 => Loss: 22.801040\n",
      "Iteration  431 => Loss: 22.764740\n",
      "Iteration  432 => Loss: 22.728640\n",
      "Iteration  433 => Loss: 22.692740\n",
      "Iteration  434 => Loss: 22.657040\n",
      "Iteration  435 => Loss: 22.621540\n",
      "Iteration  436 => Loss: 22.586240\n",
      "Iteration  437 => Loss: 22.551140\n",
      "Iteration  438 => Loss: 22.516240\n",
      "Iteration  439 => Loss: 22.481540\n",
      "Iteration  440 => Loss: 22.447040\n",
      "Iteration  441 => Loss: 22.412740\n",
      "Iteration  442 => Loss: 22.378640\n",
      "Iteration  443 => Loss: 22.344740\n",
      "Iteration  444 => Loss: 22.311040\n",
      "Iteration  445 => Loss: 22.277540\n",
      "Iteration  446 => Loss: 22.244240\n",
      "Iteration  447 => Loss: 22.243100\n",
      "Iteration  448 => Loss: 22.207400\n",
      "Iteration  449 => Loss: 22.171900\n",
      "Iteration  450 => Loss: 22.136600\n",
      "Iteration  451 => Loss: 22.101500\n",
      "Iteration  452 => Loss: 22.066600\n",
      "Iteration  453 => Loss: 22.031900\n",
      "Iteration  454 => Loss: 21.997400\n",
      "Iteration  455 => Loss: 21.963100\n",
      "Iteration  456 => Loss: 21.929000\n",
      "Iteration  457 => Loss: 21.895100\n",
      "Iteration  458 => Loss: 21.861400\n",
      "Iteration  459 => Loss: 21.827900\n",
      "Iteration  460 => Loss: 21.794600\n",
      "Iteration  461 => Loss: 21.761500\n",
      "Iteration  462 => Loss: 21.728600\n",
      "Iteration  463 => Loss: 21.695900\n",
      "Iteration  464 => Loss: 21.695840\n",
      "Iteration  465 => Loss: 21.660740\n",
      "Iteration  466 => Loss: 21.625840\n",
      "Iteration  467 => Loss: 21.591140\n",
      "Iteration  468 => Loss: 21.556640\n",
      "Iteration  469 => Loss: 21.522340\n",
      "Iteration  470 => Loss: 21.488240\n",
      "Iteration  471 => Loss: 21.454340\n",
      "Iteration  472 => Loss: 21.420640\n",
      "Iteration  473 => Loss: 21.387140\n",
      "Iteration  474 => Loss: 21.353840\n",
      "Iteration  475 => Loss: 21.320740\n",
      "Iteration  476 => Loss: 21.287840\n",
      "Iteration  477 => Loss: 21.255140\n",
      "Iteration  478 => Loss: 21.222640\n",
      "Iteration  479 => Loss: 21.190340\n",
      "Iteration  480 => Loss: 21.158240\n",
      "Iteration  481 => Loss: 21.126340\n",
      "Iteration  482 => Loss: 21.124760\n",
      "Iteration  483 => Loss: 21.090460\n",
      "Iteration  484 => Loss: 21.056360\n",
      "Iteration  485 => Loss: 21.022460\n",
      "Iteration  486 => Loss: 20.988760\n",
      "Iteration  487 => Loss: 20.955260\n",
      "Iteration  488 => Loss: 20.921960\n",
      "Iteration  489 => Loss: 20.888860\n",
      "Iteration  490 => Loss: 20.855960\n",
      "Iteration  491 => Loss: 20.823260\n",
      "Iteration  492 => Loss: 20.790760\n",
      "Iteration  493 => Loss: 20.758460\n",
      "Iteration  494 => Loss: 20.726360\n",
      "Iteration  495 => Loss: 20.694460\n",
      "Iteration  496 => Loss: 20.662760\n",
      "Iteration  497 => Loss: 20.631260\n",
      "Iteration  498 => Loss: 20.599960\n",
      "Iteration  499 => Loss: 20.599460\n",
      "Iteration  500 => Loss: 20.565760\n",
      "Iteration  501 => Loss: 20.532260\n",
      "Iteration  502 => Loss: 20.498960\n",
      "Iteration  503 => Loss: 20.465860\n",
      "Iteration  504 => Loss: 20.432960\n",
      "Iteration  505 => Loss: 20.400260\n",
      "Iteration  506 => Loss: 20.367760\n",
      "Iteration  507 => Loss: 20.335460\n",
      "Iteration  508 => Loss: 20.303360\n",
      "Iteration  509 => Loss: 20.271460\n",
      "Iteration  510 => Loss: 20.239760\n",
      "Iteration  511 => Loss: 20.208260\n",
      "Iteration  512 => Loss: 20.176960\n",
      "Iteration  513 => Loss: 20.145860\n",
      "Iteration  514 => Loss: 20.114960\n",
      "Iteration  515 => Loss: 20.084260\n",
      "Iteration  516 => Loss: 20.053760\n",
      "Iteration  517 => Loss: 20.051740\n",
      "Iteration  518 => Loss: 20.018840\n",
      "Iteration  519 => Loss: 19.986140\n",
      "Iteration  520 => Loss: 19.953640\n",
      "Iteration  521 => Loss: 19.921340\n",
      "Iteration  522 => Loss: 19.889240\n",
      "Iteration  523 => Loss: 19.857340\n",
      "Iteration  524 => Loss: 19.825640\n",
      "Iteration  525 => Loss: 19.794140\n",
      "Iteration  526 => Loss: 19.762840\n",
      "Iteration  527 => Loss: 19.731740\n",
      "Iteration  528 => Loss: 19.700840\n",
      "Iteration  529 => Loss: 19.670140\n",
      "Iteration  530 => Loss: 19.639640\n",
      "Iteration  531 => Loss: 19.609340\n",
      "Iteration  532 => Loss: 19.579240\n",
      "Iteration  533 => Loss: 19.549340\n",
      "Iteration  534 => Loss: 19.548400\n",
      "Iteration  535 => Loss: 19.516100\n",
      "Iteration  536 => Loss: 19.484000\n",
      "Iteration  537 => Loss: 19.452100\n",
      "Iteration  538 => Loss: 19.420400\n",
      "Iteration  539 => Loss: 19.388900\n",
      "Iteration  540 => Loss: 19.357600\n",
      "Iteration  541 => Loss: 19.326500\n",
      "Iteration  542 => Loss: 19.295600\n",
      "Iteration  543 => Loss: 19.264900\n",
      "Iteration  544 => Loss: 19.234400\n",
      "Iteration  545 => Loss: 19.204100\n",
      "Iteration  546 => Loss: 19.174000\n",
      "Iteration  547 => Loss: 19.144100\n",
      "Iteration  548 => Loss: 19.114400\n",
      "Iteration  549 => Loss: 19.084900\n",
      "Iteration  550 => Loss: 19.055600\n",
      "Iteration  551 => Loss: 19.026500\n",
      "Iteration  552 => Loss: 19.024040\n",
      "Iteration  553 => Loss: 18.992540\n",
      "Iteration  554 => Loss: 18.961240\n",
      "Iteration  555 => Loss: 18.930140\n",
      "Iteration  556 => Loss: 18.899240\n",
      "Iteration  557 => Loss: 18.868540\n",
      "Iteration  558 => Loss: 18.838040\n",
      "Iteration  559 => Loss: 18.807740\n",
      "Iteration  560 => Loss: 18.777640\n",
      "Iteration  561 => Loss: 18.747740\n",
      "Iteration  562 => Loss: 18.718040\n",
      "Iteration  563 => Loss: 18.688540\n",
      "Iteration  564 => Loss: 18.659240\n",
      "Iteration  565 => Loss: 18.630140\n",
      "Iteration  566 => Loss: 18.601240\n",
      "Iteration  567 => Loss: 18.572540\n",
      "Iteration  568 => Loss: 18.544040\n",
      "Iteration  569 => Loss: 18.542660\n",
      "Iteration  570 => Loss: 18.511760\n",
      "Iteration  571 => Loss: 18.481060\n",
      "Iteration  572 => Loss: 18.450560\n",
      "Iteration  573 => Loss: 18.420260\n",
      "Iteration  574 => Loss: 18.390160\n",
      "Iteration  575 => Loss: 18.360260\n",
      "Iteration  576 => Loss: 18.330560\n",
      "Iteration  577 => Loss: 18.301060\n",
      "Iteration  578 => Loss: 18.271760\n",
      "Iteration  579 => Loss: 18.242660\n",
      "Iteration  580 => Loss: 18.213760\n",
      "Iteration  581 => Loss: 18.185060\n",
      "Iteration  582 => Loss: 18.156560\n",
      "Iteration  583 => Loss: 18.128260\n",
      "Iteration  584 => Loss: 18.100160\n",
      "Iteration  585 => Loss: 18.072260\n",
      "Iteration  586 => Loss: 18.071960\n",
      "Iteration  587 => Loss: 18.041660\n",
      "Iteration  588 => Loss: 18.011560\n",
      "Iteration  589 => Loss: 17.981660\n",
      "Iteration  590 => Loss: 17.951960\n",
      "Iteration  591 => Loss: 17.922460\n",
      "Iteration  592 => Loss: 17.893160\n",
      "Iteration  593 => Loss: 17.864060\n",
      "Iteration  594 => Loss: 17.835160\n",
      "Iteration  595 => Loss: 17.806460\n",
      "Iteration  596 => Loss: 17.777960\n",
      "Iteration  597 => Loss: 17.749660\n",
      "Iteration  598 => Loss: 17.721560\n",
      "Iteration  599 => Loss: 17.693660\n",
      "Iteration  600 => Loss: 17.665960\n",
      "Iteration  601 => Loss: 17.638460\n",
      "Iteration  602 => Loss: 17.611160\n",
      "Iteration  603 => Loss: 17.584060\n",
      "Iteration  604 => Loss: 17.582240\n",
      "Iteration  605 => Loss: 17.552740\n",
      "Iteration  606 => Loss: 17.523440\n",
      "Iteration  607 => Loss: 17.494340\n",
      "Iteration  608 => Loss: 17.465440\n",
      "Iteration  609 => Loss: 17.436740\n",
      "Iteration  610 => Loss: 17.408240\n",
      "Iteration  611 => Loss: 17.379940\n",
      "Iteration  612 => Loss: 17.351840\n",
      "Iteration  613 => Loss: 17.323940\n",
      "Iteration  614 => Loss: 17.296240\n",
      "Iteration  615 => Loss: 17.268740\n",
      "Iteration  616 => Loss: 17.241440\n",
      "Iteration  617 => Loss: 17.214340\n",
      "Iteration  618 => Loss: 17.187440\n",
      "Iteration  619 => Loss: 17.160740\n",
      "Iteration  620 => Loss: 17.134240\n",
      "Iteration  621 => Loss: 17.133500\n",
      "Iteration  622 => Loss: 17.104600\n",
      "Iteration  623 => Loss: 17.075900\n",
      "Iteration  624 => Loss: 17.047400\n",
      "Iteration  625 => Loss: 17.019100\n",
      "Iteration  626 => Loss: 16.991000\n",
      "Iteration  627 => Loss: 16.963100\n",
      "Iteration  628 => Loss: 16.935400\n",
      "Iteration  629 => Loss: 16.907900\n",
      "Iteration  630 => Loss: 16.880600\n",
      "Iteration  631 => Loss: 16.853500\n",
      "Iteration  632 => Loss: 16.826600\n",
      "Iteration  633 => Loss: 16.799900\n",
      "Iteration  634 => Loss: 16.773400\n",
      "Iteration  635 => Loss: 16.747100\n",
      "Iteration  636 => Loss: 16.721000\n",
      "Iteration  637 => Loss: 16.695100\n",
      "Iteration  638 => Loss: 16.669400\n",
      "Iteration  639 => Loss: 16.667140\n",
      "Iteration  640 => Loss: 16.639040\n",
      "Iteration  641 => Loss: 16.611140\n",
      "Iteration  642 => Loss: 16.583440\n",
      "Iteration  643 => Loss: 16.555940\n",
      "Iteration  644 => Loss: 16.528640\n",
      "Iteration  645 => Loss: 16.501540\n",
      "Iteration  646 => Loss: 16.474640\n",
      "Iteration  647 => Loss: 16.447940\n",
      "Iteration  648 => Loss: 16.421440\n",
      "Iteration  649 => Loss: 16.395140\n",
      "Iteration  650 => Loss: 16.369040\n",
      "Iteration  651 => Loss: 16.343140\n",
      "Iteration  652 => Loss: 16.317440\n",
      "Iteration  653 => Loss: 16.291940\n",
      "Iteration  654 => Loss: 16.266640\n",
      "Iteration  655 => Loss: 16.241540\n",
      "Iteration  656 => Loss: 16.240360\n",
      "Iteration  657 => Loss: 16.212860\n",
      "Iteration  658 => Loss: 16.185560\n",
      "Iteration  659 => Loss: 16.158460\n",
      "Iteration  660 => Loss: 16.131560\n",
      "Iteration  661 => Loss: 16.104860\n",
      "Iteration  662 => Loss: 16.078360\n",
      "Iteration  663 => Loss: 16.052060\n",
      "Iteration  664 => Loss: 16.025960\n",
      "Iteration  665 => Loss: 16.000060\n",
      "Iteration  666 => Loss: 15.974360\n",
      "Iteration  667 => Loss: 15.948860\n",
      "Iteration  668 => Loss: 15.923560\n",
      "Iteration  669 => Loss: 15.898460\n",
      "Iteration  670 => Loss: 15.873560\n",
      "Iteration  671 => Loss: 15.848860\n",
      "Iteration  672 => Loss: 15.824360\n",
      "Iteration  673 => Loss: 15.824260\n",
      "Iteration  674 => Loss: 15.797360\n",
      "Iteration  675 => Loss: 15.770660\n",
      "Iteration  676 => Loss: 15.744160\n",
      "Iteration  677 => Loss: 15.717860\n",
      "Iteration  678 => Loss: 15.691760\n",
      "Iteration  679 => Loss: 15.665860\n",
      "Iteration  680 => Loss: 15.640160\n",
      "Iteration  681 => Loss: 15.614660\n",
      "Iteration  682 => Loss: 15.589360\n",
      "Iteration  683 => Loss: 15.564260\n",
      "Iteration  684 => Loss: 15.539360\n",
      "Iteration  685 => Loss: 15.514660\n",
      "Iteration  686 => Loss: 15.490160\n",
      "Iteration  687 => Loss: 15.465860\n",
      "Iteration  688 => Loss: 15.441760\n",
      "Iteration  689 => Loss: 15.417860\n",
      "Iteration  690 => Loss: 15.394160\n",
      "Iteration  691 => Loss: 15.392540\n",
      "Iteration  692 => Loss: 15.366440\n",
      "Iteration  693 => Loss: 15.340540\n",
      "Iteration  694 => Loss: 15.314840\n",
      "Iteration  695 => Loss: 15.289340\n",
      "Iteration  696 => Loss: 15.264040\n",
      "Iteration  697 => Loss: 15.238940\n",
      "Iteration  698 => Loss: 15.214040\n",
      "Iteration  699 => Loss: 15.189340\n",
      "Iteration  700 => Loss: 15.164840\n",
      "Iteration  701 => Loss: 15.140540\n",
      "Iteration  702 => Loss: 15.116440\n",
      "Iteration  703 => Loss: 15.092540\n",
      "Iteration  704 => Loss: 15.068840\n",
      "Iteration  705 => Loss: 15.045340\n",
      "Iteration  706 => Loss: 15.022040\n",
      "Iteration  707 => Loss: 14.998940\n",
      "Iteration  708 => Loss: 14.998400\n",
      "Iteration  709 => Loss: 14.972900\n",
      "Iteration  710 => Loss: 14.947600\n",
      "Iteration  711 => Loss: 14.922500\n",
      "Iteration  712 => Loss: 14.897600\n",
      "Iteration  713 => Loss: 14.872900\n",
      "Iteration  714 => Loss: 14.848400\n",
      "Iteration  715 => Loss: 14.824100\n",
      "Iteration  716 => Loss: 14.800000\n",
      "Iteration  717 => Loss: 14.776100\n",
      "Iteration  718 => Loss: 14.752400\n",
      "Iteration  719 => Loss: 14.728900\n",
      "Iteration  720 => Loss: 14.705600\n",
      "Iteration  721 => Loss: 14.682500\n",
      "Iteration  722 => Loss: 14.659600\n",
      "Iteration  723 => Loss: 14.636900\n",
      "Iteration  724 => Loss: 14.614400\n",
      "Iteration  725 => Loss: 14.592100\n",
      "Iteration  726 => Loss: 14.590040\n",
      "Iteration  727 => Loss: 14.565340\n",
      "Iteration  728 => Loss: 14.540840\n",
      "Iteration  729 => Loss: 14.516540\n",
      "Iteration  730 => Loss: 14.492440\n",
      "Iteration  731 => Loss: 14.468540\n",
      "Iteration  732 => Loss: 14.444840\n",
      "Iteration  733 => Loss: 14.421340\n",
      "Iteration  734 => Loss: 14.398040\n",
      "Iteration  735 => Loss: 14.374940\n",
      "Iteration  736 => Loss: 14.352040\n",
      "Iteration  737 => Loss: 14.329340\n",
      "Iteration  738 => Loss: 14.306840\n",
      "Iteration  739 => Loss: 14.284540\n",
      "Iteration  740 => Loss: 14.262440\n",
      "Iteration  741 => Loss: 14.240540\n",
      "Iteration  742 => Loss: 14.218840\n",
      "Iteration  743 => Loss: 14.217860\n",
      "Iteration  744 => Loss: 14.193760\n",
      "Iteration  745 => Loss: 14.169860\n",
      "Iteration  746 => Loss: 14.146160\n",
      "Iteration  747 => Loss: 14.122660\n",
      "Iteration  748 => Loss: 14.099360\n",
      "Iteration  749 => Loss: 14.076260\n",
      "Iteration  750 => Loss: 14.053360\n",
      "Iteration  751 => Loss: 14.030660\n",
      "Iteration  752 => Loss: 14.008160\n",
      "Iteration  753 => Loss: 13.985860\n",
      "Iteration  754 => Loss: 13.963760\n",
      "Iteration  755 => Loss: 13.941860\n",
      "Iteration  756 => Loss: 13.920160\n",
      "Iteration  757 => Loss: 13.898660\n",
      "Iteration  758 => Loss: 13.877360\n",
      "Iteration  759 => Loss: 13.856260\n",
      "Iteration  760 => Loss: 13.835360\n",
      "Iteration  761 => Loss: 13.832860\n",
      "Iteration  762 => Loss: 13.809560\n",
      "Iteration  763 => Loss: 13.786460\n",
      "Iteration  764 => Loss: 13.763560\n",
      "Iteration  765 => Loss: 13.740860\n",
      "Iteration  766 => Loss: 13.718360\n",
      "Iteration  767 => Loss: 13.696060\n",
      "Iteration  768 => Loss: 13.673960\n",
      "Iteration  769 => Loss: 13.652060\n",
      "Iteration  770 => Loss: 13.630360\n",
      "Iteration  771 => Loss: 13.608860\n",
      "Iteration  772 => Loss: 13.587560\n",
      "Iteration  773 => Loss: 13.566460\n",
      "Iteration  774 => Loss: 13.545560\n",
      "Iteration  775 => Loss: 13.524860\n",
      "Iteration  776 => Loss: 13.504360\n",
      "Iteration  777 => Loss: 13.484060\n",
      "Iteration  778 => Loss: 13.482640\n",
      "Iteration  779 => Loss: 13.459940\n",
      "Iteration  780 => Loss: 13.437440\n",
      "Iteration  781 => Loss: 13.415140\n",
      "Iteration  782 => Loss: 13.393040\n",
      "Iteration  783 => Loss: 13.371140\n",
      "Iteration  784 => Loss: 13.349440\n",
      "Iteration  785 => Loss: 13.327940\n",
      "Iteration  786 => Loss: 13.306640\n",
      "Iteration  787 => Loss: 13.285540\n",
      "Iteration  788 => Loss: 13.264640\n",
      "Iteration  789 => Loss: 13.243940\n",
      "Iteration  790 => Loss: 13.223440\n",
      "Iteration  791 => Loss: 13.203140\n",
      "Iteration  792 => Loss: 13.183040\n",
      "Iteration  793 => Loss: 13.163140\n",
      "Iteration  794 => Loss: 13.143440\n",
      "Iteration  795 => Loss: 13.143100\n",
      "Iteration  796 => Loss: 13.121000\n",
      "Iteration  797 => Loss: 13.099100\n",
      "Iteration  798 => Loss: 13.077400\n",
      "Iteration  799 => Loss: 13.055900\n",
      "Iteration  800 => Loss: 13.034600\n",
      "Iteration  801 => Loss: 13.013500\n",
      "Iteration  802 => Loss: 12.992600\n",
      "Iteration  803 => Loss: 12.971900\n",
      "Iteration  804 => Loss: 12.951400\n",
      "Iteration  805 => Loss: 12.931100\n",
      "Iteration  806 => Loss: 12.911000\n",
      "Iteration  807 => Loss: 12.891100\n",
      "Iteration  808 => Loss: 12.871400\n",
      "Iteration  809 => Loss: 12.851900\n",
      "Iteration  810 => Loss: 12.832600\n",
      "Iteration  811 => Loss: 12.813500\n",
      "Iteration  812 => Loss: 12.794600\n",
      "Iteration  813 => Loss: 12.792740\n",
      "Iteration  814 => Loss: 12.771440\n",
      "Iteration  815 => Loss: 12.750340\n",
      "Iteration  816 => Loss: 12.729440\n",
      "Iteration  817 => Loss: 12.708740\n",
      "Iteration  818 => Loss: 12.688240\n",
      "Iteration  819 => Loss: 12.667940\n",
      "Iteration  820 => Loss: 12.647840\n",
      "Iteration  821 => Loss: 12.627940\n",
      "Iteration  822 => Loss: 12.608240\n",
      "Iteration  823 => Loss: 12.588740\n",
      "Iteration  824 => Loss: 12.569440\n",
      "Iteration  825 => Loss: 12.550340\n",
      "Iteration  826 => Loss: 12.531440\n",
      "Iteration  827 => Loss: 12.512740\n",
      "Iteration  828 => Loss: 12.494240\n",
      "Iteration  829 => Loss: 12.475940\n",
      "Iteration  830 => Loss: 12.475160\n",
      "Iteration  831 => Loss: 12.454460\n",
      "Iteration  832 => Loss: 12.433960\n",
      "Iteration  833 => Loss: 12.413660\n",
      "Iteration  834 => Loss: 12.393560\n",
      "Iteration  835 => Loss: 12.373660\n",
      "Iteration  836 => Loss: 12.353960\n",
      "Iteration  837 => Loss: 12.334460\n",
      "Iteration  838 => Loss: 12.315160\n",
      "Iteration  839 => Loss: 12.296060\n",
      "Iteration  840 => Loss: 12.277160\n",
      "Iteration  841 => Loss: 12.258460\n",
      "Iteration  842 => Loss: 12.239960\n",
      "Iteration  843 => Loss: 12.221660\n",
      "Iteration  844 => Loss: 12.203560\n",
      "Iteration  845 => Loss: 12.185660\n",
      "Iteration  846 => Loss: 12.167960\n",
      "Iteration  847 => Loss: 12.150460\n",
      "Iteration  848 => Loss: 12.148160\n",
      "Iteration  849 => Loss: 12.128260\n",
      "Iteration  850 => Loss: 12.108560\n",
      "Iteration  851 => Loss: 12.089060\n",
      "Iteration  852 => Loss: 12.069760\n",
      "Iteration  853 => Loss: 12.050660\n",
      "Iteration  854 => Loss: 12.031760\n",
      "Iteration  855 => Loss: 12.013060\n",
      "Iteration  856 => Loss: 11.994560\n",
      "Iteration  857 => Loss: 11.976260\n",
      "Iteration  858 => Loss: 11.958160\n",
      "Iteration  859 => Loss: 11.940260\n",
      "Iteration  860 => Loss: 11.922560\n",
      "Iteration  861 => Loss: 11.905060\n",
      "Iteration  862 => Loss: 11.887760\n",
      "Iteration  863 => Loss: 11.870660\n",
      "Iteration  864 => Loss: 11.853760\n",
      "Iteration  865 => Loss: 11.852540\n",
      "Iteration  866 => Loss: 11.833240\n",
      "Iteration  867 => Loss: 11.814140\n",
      "Iteration  868 => Loss: 11.795240\n",
      "Iteration  869 => Loss: 11.776540\n",
      "Iteration  870 => Loss: 11.758040\n",
      "Iteration  871 => Loss: 11.739740\n",
      "Iteration  872 => Loss: 11.721640\n",
      "Iteration  873 => Loss: 11.703740\n",
      "Iteration  874 => Loss: 11.686040\n",
      "Iteration  875 => Loss: 11.668540\n",
      "Iteration  876 => Loss: 11.651240\n",
      "Iteration  877 => Loss: 11.634140\n",
      "Iteration  878 => Loss: 11.617240\n",
      "Iteration  879 => Loss: 11.600540\n",
      "Iteration  880 => Loss: 11.584040\n",
      "Iteration  881 => Loss: 11.567740\n",
      "Iteration  882 => Loss: 11.567600\n",
      "Iteration  883 => Loss: 11.548900\n",
      "Iteration  884 => Loss: 11.530400\n",
      "Iteration  885 => Loss: 11.512100\n",
      "Iteration  886 => Loss: 11.494000\n",
      "Iteration  887 => Loss: 11.476100\n",
      "Iteration  888 => Loss: 11.458400\n",
      "Iteration  889 => Loss: 11.440900\n",
      "Iteration  890 => Loss: 11.423600\n",
      "Iteration  891 => Loss: 11.406500\n",
      "Iteration  892 => Loss: 11.389600\n",
      "Iteration  893 => Loss: 11.372900\n",
      "Iteration  894 => Loss: 11.356400\n",
      "Iteration  895 => Loss: 11.340100\n",
      "Iteration  896 => Loss: 11.324000\n",
      "Iteration  897 => Loss: 11.308100\n",
      "Iteration  898 => Loss: 11.292400\n",
      "Iteration  899 => Loss: 11.276900\n",
      "Iteration  900 => Loss: 11.275240\n",
      "Iteration  901 => Loss: 11.257340\n",
      "Iteration  902 => Loss: 11.239640\n",
      "Iteration  903 => Loss: 11.222140\n",
      "Iteration  904 => Loss: 11.204840\n",
      "Iteration  905 => Loss: 11.187740\n",
      "Iteration  906 => Loss: 11.170840\n",
      "Iteration  907 => Loss: 11.154140\n",
      "Iteration  908 => Loss: 11.137640\n",
      "Iteration  909 => Loss: 11.121340\n",
      "Iteration  910 => Loss: 11.105240\n",
      "Iteration  911 => Loss: 11.089340\n",
      "Iteration  912 => Loss: 11.073640\n",
      "Iteration  913 => Loss: 11.058140\n",
      "Iteration  914 => Loss: 11.042840\n",
      "Iteration  915 => Loss: 11.027740\n",
      "Iteration  916 => Loss: 11.012840\n",
      "Iteration  917 => Loss: 11.012260\n",
      "Iteration  918 => Loss: 10.994960\n",
      "Iteration  919 => Loss: 10.977860\n",
      "Iteration  920 => Loss: 10.960960\n",
      "Iteration  921 => Loss: 10.944260\n",
      "Iteration  922 => Loss: 10.927760\n",
      "Iteration  923 => Loss: 10.911460\n",
      "Iteration  924 => Loss: 10.895360\n",
      "Iteration  925 => Loss: 10.879460\n",
      "Iteration  926 => Loss: 10.863760\n",
      "Iteration  927 => Loss: 10.848260\n",
      "Iteration  928 => Loss: 10.832960\n",
      "Iteration  929 => Loss: 10.817860\n",
      "Iteration  930 => Loss: 10.802960\n",
      "Iteration  931 => Loss: 10.788260\n",
      "Iteration  932 => Loss: 10.773760\n",
      "Iteration  933 => Loss: 10.759460\n",
      "Iteration  934 => Loss: 10.745360\n",
      "Iteration  935 => Loss: 10.743260\n",
      "Iteration  936 => Loss: 10.726760\n",
      "Iteration  937 => Loss: 10.710460\n",
      "Iteration  938 => Loss: 10.694360\n",
      "Iteration  939 => Loss: 10.678460\n",
      "Iteration  940 => Loss: 10.662760\n",
      "Iteration  941 => Loss: 10.647260\n",
      "Iteration  942 => Loss: 10.631960\n",
      "Iteration  943 => Loss: 10.616860\n",
      "Iteration  944 => Loss: 10.601960\n",
      "Iteration  945 => Loss: 10.587260\n",
      "Iteration  946 => Loss: 10.572760\n",
      "Iteration  947 => Loss: 10.558460\n",
      "Iteration  948 => Loss: 10.544360\n",
      "Iteration  949 => Loss: 10.530460\n",
      "Iteration  950 => Loss: 10.516760\n",
      "Iteration  951 => Loss: 10.503260\n",
      "Iteration  952 => Loss: 10.502240\n",
      "Iteration  953 => Loss: 10.486340\n",
      "Iteration  954 => Loss: 10.470640\n",
      "Iteration  955 => Loss: 10.455140\n",
      "Iteration  956 => Loss: 10.439840\n",
      "Iteration  957 => Loss: 10.424740\n",
      "Iteration  958 => Loss: 10.409840\n",
      "Iteration  959 => Loss: 10.395140\n",
      "Iteration  960 => Loss: 10.380640\n",
      "Iteration  961 => Loss: 10.366340\n",
      "Iteration  962 => Loss: 10.352240\n",
      "Iteration  963 => Loss: 10.338340\n",
      "Iteration  964 => Loss: 10.324640\n",
      "Iteration  965 => Loss: 10.311140\n",
      "Iteration  966 => Loss: 10.297840\n",
      "Iteration  967 => Loss: 10.284740\n",
      "Iteration  968 => Loss: 10.271840\n",
      "Iteration  969 => Loss: 10.259140\n",
      "Iteration  970 => Loss: 10.256600\n",
      "Iteration  971 => Loss: 10.241500\n",
      "Iteration  972 => Loss: 10.226600\n",
      "Iteration  973 => Loss: 10.211900\n",
      "Iteration  974 => Loss: 10.197400\n",
      "Iteration  975 => Loss: 10.183100\n",
      "Iteration  976 => Loss: 10.169000\n",
      "Iteration  977 => Loss: 10.155100\n",
      "Iteration  978 => Loss: 10.141400\n",
      "Iteration  979 => Loss: 10.127900\n",
      "Iteration  980 => Loss: 10.114600\n",
      "Iteration  981 => Loss: 10.101500\n",
      "Iteration  982 => Loss: 10.088600\n",
      "Iteration  983 => Loss: 10.075900\n",
      "Iteration  984 => Loss: 10.063400\n",
      "Iteration  985 => Loss: 10.051100\n",
      "Iteration  986 => Loss: 10.039000\n",
      "Iteration  987 => Loss: 10.037540\n",
      "Iteration  988 => Loss: 10.023040\n",
      "Iteration  989 => Loss: 10.008740\n",
      "Iteration  990 => Loss: 9.994640\n",
      "Iteration  991 => Loss: 9.980740\n",
      "Iteration  992 => Loss: 9.967040\n",
      "Iteration  993 => Loss: 9.953540\n",
      "Iteration  994 => Loss: 9.940240\n",
      "Iteration  995 => Loss: 9.927140\n",
      "Iteration  996 => Loss: 9.914240\n",
      "Iteration  997 => Loss: 9.901540\n",
      "Iteration  998 => Loss: 9.889040\n",
      "Iteration  999 => Loss: 9.876740\n",
      "Iteration 1000 => Loss: 9.864640\n",
      "Iteration 1001 => Loss: 9.852740\n",
      "Iteration 1002 => Loss: 9.841040\n",
      "Iteration 1003 => Loss: 9.829540\n",
      "Iteration 1004 => Loss: 9.829160\n",
      "Iteration 1005 => Loss: 9.815260\n",
      "Iteration 1006 => Loss: 9.801560\n",
      "Iteration 1007 => Loss: 9.788060\n",
      "Iteration 1008 => Loss: 9.774760\n",
      "Iteration 1009 => Loss: 9.761660\n",
      "Iteration 1010 => Loss: 9.748760\n",
      "Iteration 1011 => Loss: 9.736060\n",
      "Iteration 1012 => Loss: 9.723560\n",
      "Iteration 1013 => Loss: 9.711260\n",
      "Iteration 1014 => Loss: 9.699160\n",
      "Iteration 1015 => Loss: 9.687260\n",
      "Iteration 1016 => Loss: 9.675560\n",
      "Iteration 1017 => Loss: 9.664060\n",
      "Iteration 1018 => Loss: 9.652760\n",
      "Iteration 1019 => Loss: 9.641660\n",
      "Iteration 1020 => Loss: 9.630760\n",
      "Iteration 1021 => Loss: 9.620060\n",
      "Iteration 1022 => Loss: 9.618160\n",
      "Iteration 1023 => Loss: 9.605060\n",
      "Iteration 1024 => Loss: 9.592160\n",
      "Iteration 1025 => Loss: 9.579460\n",
      "Iteration 1026 => Loss: 9.566960\n",
      "Iteration 1027 => Loss: 9.554660\n",
      "Iteration 1028 => Loss: 9.542560\n",
      "Iteration 1029 => Loss: 9.530660\n",
      "Iteration 1030 => Loss: 9.518960\n",
      "Iteration 1031 => Loss: 9.507460\n",
      "Iteration 1032 => Loss: 9.496160\n",
      "Iteration 1033 => Loss: 9.485060\n",
      "Iteration 1034 => Loss: 9.474160\n",
      "Iteration 1035 => Loss: 9.463460\n",
      "Iteration 1036 => Loss: 9.452960\n",
      "Iteration 1037 => Loss: 9.442660\n",
      "Iteration 1038 => Loss: 9.432560\n",
      "Iteration 1039 => Loss: 9.431740\n",
      "Iteration 1040 => Loss: 9.419240\n",
      "Iteration 1041 => Loss: 9.406940\n",
      "Iteration 1042 => Loss: 9.394840\n",
      "Iteration 1043 => Loss: 9.382940\n",
      "Iteration 1044 => Loss: 9.371240\n",
      "Iteration 1045 => Loss: 9.359740\n",
      "Iteration 1046 => Loss: 9.348440\n",
      "Iteration 1047 => Loss: 9.337340\n",
      "Iteration 1048 => Loss: 9.326440\n",
      "Iteration 1049 => Loss: 9.315740\n",
      "Iteration 1050 => Loss: 9.305240\n",
      "Iteration 1051 => Loss: 9.294940\n",
      "Iteration 1052 => Loss: 9.284840\n",
      "Iteration 1053 => Loss: 9.274940\n",
      "Iteration 1054 => Loss: 9.265240\n",
      "Iteration 1055 => Loss: 9.255740\n",
      "Iteration 1056 => Loss: 9.246440\n",
      "Iteration 1057 => Loss: 9.244100\n",
      "Iteration 1058 => Loss: 9.232400\n",
      "Iteration 1059 => Loss: 9.220900\n",
      "Iteration 1060 => Loss: 9.209600\n",
      "Iteration 1061 => Loss: 9.198500\n",
      "Iteration 1062 => Loss: 9.187600\n",
      "Iteration 1063 => Loss: 9.176900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1064 => Loss: 9.166400\n",
      "Iteration 1065 => Loss: 9.156100\n",
      "Iteration 1066 => Loss: 9.146000\n",
      "Iteration 1067 => Loss: 9.136100\n",
      "Iteration 1068 => Loss: 9.126400\n",
      "Iteration 1069 => Loss: 9.116900\n",
      "Iteration 1070 => Loss: 9.107600\n",
      "Iteration 1071 => Loss: 9.098500\n",
      "Iteration 1072 => Loss: 9.089600\n",
      "Iteration 1073 => Loss: 9.080900\n",
      "Iteration 1074 => Loss: 9.079640\n",
      "Iteration 1075 => Loss: 9.068540\n",
      "Iteration 1076 => Loss: 9.057640\n",
      "Iteration 1077 => Loss: 9.046940\n",
      "Iteration 1078 => Loss: 9.036440\n",
      "Iteration 1079 => Loss: 9.026140\n",
      "Iteration 1080 => Loss: 9.016040\n",
      "Iteration 1081 => Loss: 9.006140\n",
      "Iteration 1082 => Loss: 8.996440\n",
      "Iteration 1083 => Loss: 8.986940\n",
      "Iteration 1084 => Loss: 8.977640\n",
      "Iteration 1085 => Loss: 8.968540\n",
      "Iteration 1086 => Loss: 8.959640\n",
      "Iteration 1087 => Loss: 8.950940\n",
      "Iteration 1088 => Loss: 8.942440\n",
      "Iteration 1089 => Loss: 8.934140\n",
      "Iteration 1090 => Loss: 8.926040\n",
      "Iteration 1091 => Loss: 8.925860\n",
      "Iteration 1092 => Loss: 8.915360\n",
      "Iteration 1093 => Loss: 8.905060\n",
      "Iteration 1094 => Loss: 8.894960\n",
      "Iteration 1095 => Loss: 8.885060\n",
      "Iteration 1096 => Loss: 8.875360\n",
      "Iteration 1097 => Loss: 8.865860\n",
      "Iteration 1098 => Loss: 8.856560\n",
      "Iteration 1099 => Loss: 8.847460\n",
      "Iteration 1100 => Loss: 8.838560\n",
      "Iteration 1101 => Loss: 8.829860\n",
      "Iteration 1102 => Loss: 8.821360\n",
      "Iteration 1103 => Loss: 8.813060\n",
      "Iteration 1104 => Loss: 8.804960\n",
      "Iteration 1105 => Loss: 8.797060\n",
      "Iteration 1106 => Loss: 8.789360\n",
      "Iteration 1107 => Loss: 8.781860\n",
      "Iteration 1108 => Loss: 8.774560\n",
      "Iteration 1109 => Loss: 8.772860\n",
      "Iteration 1110 => Loss: 8.763160\n",
      "Iteration 1111 => Loss: 8.753660\n",
      "Iteration 1112 => Loss: 8.744360\n",
      "Iteration 1113 => Loss: 8.735260\n",
      "Iteration 1114 => Loss: 8.726360\n",
      "Iteration 1115 => Loss: 8.717660\n",
      "Iteration 1116 => Loss: 8.709160\n",
      "Iteration 1117 => Loss: 8.700860\n",
      "Iteration 1118 => Loss: 8.692760\n",
      "Iteration 1119 => Loss: 8.684860\n",
      "Iteration 1120 => Loss: 8.677160\n",
      "Iteration 1121 => Loss: 8.669660\n",
      "Iteration 1122 => Loss: 8.662360\n",
      "Iteration 1123 => Loss: 8.655260\n",
      "Iteration 1124 => Loss: 8.648360\n",
      "Iteration 1125 => Loss: 8.641660\n",
      "Iteration 1126 => Loss: 8.641040\n",
      "Iteration 1127 => Loss: 8.631940\n",
      "Iteration 1128 => Loss: 8.623040\n",
      "Iteration 1129 => Loss: 8.614340\n",
      "Iteration 1130 => Loss: 8.605840\n",
      "Iteration 1131 => Loss: 8.597540\n",
      "Iteration 1132 => Loss: 8.589440\n",
      "Iteration 1133 => Loss: 8.581540\n",
      "Iteration 1134 => Loss: 8.573840\n",
      "Iteration 1135 => Loss: 8.566340\n",
      "Iteration 1136 => Loss: 8.559040\n",
      "Iteration 1137 => Loss: 8.551940\n",
      "Iteration 1138 => Loss: 8.545040\n",
      "Iteration 1139 => Loss: 8.538340\n",
      "Iteration 1140 => Loss: 8.531840\n",
      "Iteration 1141 => Loss: 8.525540\n",
      "Iteration 1142 => Loss: 8.519440\n",
      "Iteration 1143 => Loss: 8.513540\n",
      "Iteration 1144 => Loss: 8.511400\n",
      "Iteration 1145 => Loss: 8.503100\n",
      "Iteration 1146 => Loss: 8.495000\n",
      "Iteration 1147 => Loss: 8.487100\n",
      "Iteration 1148 => Loss: 8.479400\n",
      "Iteration 1149 => Loss: 8.471900\n",
      "Iteration 1150 => Loss: 8.464600\n",
      "Iteration 1151 => Loss: 8.457500\n",
      "Iteration 1152 => Loss: 8.450600\n",
      "Iteration 1153 => Loss: 8.443900\n",
      "Iteration 1154 => Loss: 8.437400\n",
      "Iteration 1155 => Loss: 8.431100\n",
      "Iteration 1156 => Loss: 8.425000\n",
      "Iteration 1157 => Loss: 8.419100\n",
      "Iteration 1158 => Loss: 8.413400\n",
      "Iteration 1159 => Loss: 8.407900\n",
      "Iteration 1160 => Loss: 8.402600\n",
      "Iteration 1161 => Loss: 8.401540\n",
      "Iteration 1162 => Loss: 8.393840\n",
      "Iteration 1163 => Loss: 8.386340\n",
      "Iteration 1164 => Loss: 8.379040\n",
      "Iteration 1165 => Loss: 8.371940\n",
      "Iteration 1166 => Loss: 8.365040\n",
      "Iteration 1167 => Loss: 8.358340\n",
      "Iteration 1168 => Loss: 8.351840\n",
      "Iteration 1169 => Loss: 8.345540\n",
      "Iteration 1170 => Loss: 8.339440\n",
      "Iteration 1171 => Loss: 8.333540\n",
      "Iteration 1172 => Loss: 8.327840\n",
      "Iteration 1173 => Loss: 8.322340\n",
      "Iteration 1174 => Loss: 8.317040\n",
      "Iteration 1175 => Loss: 8.311940\n",
      "Iteration 1176 => Loss: 8.307040\n",
      "Iteration 1177 => Loss: 8.302340\n",
      "Iteration 1178 => Loss: 8.297840\n",
      "Iteration 1179 => Loss: 8.295260\n",
      "Iteration 1180 => Loss: 8.288360\n",
      "Iteration 1181 => Loss: 8.281660\n",
      "Iteration 1182 => Loss: 8.275160\n",
      "Iteration 1183 => Loss: 8.268860\n",
      "Iteration 1184 => Loss: 8.262760\n",
      "Iteration 1185 => Loss: 8.256860\n",
      "Iteration 1186 => Loss: 8.251160\n",
      "Iteration 1187 => Loss: 8.245660\n",
      "Iteration 1188 => Loss: 8.240360\n",
      "Iteration 1189 => Loss: 8.235260\n",
      "Iteration 1190 => Loss: 8.230360\n",
      "Iteration 1191 => Loss: 8.225660\n",
      "Iteration 1192 => Loss: 8.221160\n",
      "Iteration 1193 => Loss: 8.216860\n",
      "Iteration 1194 => Loss: 8.212760\n",
      "Iteration 1195 => Loss: 8.208860\n",
      "Iteration 1196 => Loss: 8.207360\n",
      "Iteration 1197 => Loss: 8.201060\n",
      "Iteration 1198 => Loss: 8.194960\n",
      "Iteration 1199 => Loss: 8.189060\n",
      "Iteration 1200 => Loss: 8.183360\n",
      "Iteration 1201 => Loss: 8.177860\n",
      "Iteration 1202 => Loss: 8.172560\n",
      "Iteration 1203 => Loss: 8.167460\n",
      "Iteration 1204 => Loss: 8.162560\n",
      "Iteration 1205 => Loss: 8.157860\n",
      "Iteration 1206 => Loss: 8.153360\n",
      "Iteration 1207 => Loss: 8.149060\n",
      "Iteration 1208 => Loss: 8.144960\n",
      "Iteration 1209 => Loss: 8.141060\n",
      "Iteration 1210 => Loss: 8.137360\n",
      "Iteration 1211 => Loss: 8.133860\n",
      "Iteration 1212 => Loss: 8.130560\n",
      "Iteration 1213 => Loss: 8.130140\n",
      "Iteration 1214 => Loss: 8.124440\n",
      "Iteration 1215 => Loss: 8.118940\n",
      "Iteration 1216 => Loss: 8.113640\n",
      "Iteration 1217 => Loss: 8.108540\n",
      "Iteration 1218 => Loss: 8.103640\n",
      "Iteration 1219 => Loss: 8.098940\n",
      "Iteration 1220 => Loss: 8.094440\n",
      "Iteration 1221 => Loss: 8.090140\n",
      "Iteration 1222 => Loss: 8.086040\n",
      "Iteration 1223 => Loss: 8.082140\n",
      "Iteration 1224 => Loss: 8.078440\n",
      "Iteration 1225 => Loss: 8.074940\n",
      "Iteration 1226 => Loss: 8.071640\n",
      "Iteration 1227 => Loss: 8.068540\n",
      "Iteration 1228 => Loss: 8.065640\n",
      "Iteration 1229 => Loss: 8.062940\n",
      "Iteration 1230 => Loss: 8.060440\n",
      "Iteration 1231 => Loss: 8.058500\n",
      "Iteration 1232 => Loss: 8.053600\n",
      "Iteration 1233 => Loss: 8.048900\n",
      "Iteration 1234 => Loss: 8.044400\n",
      "Iteration 1235 => Loss: 8.040100\n",
      "Iteration 1236 => Loss: 8.036000\n",
      "Iteration 1237 => Loss: 8.032100\n",
      "Iteration 1238 => Loss: 8.028400\n",
      "Iteration 1239 => Loss: 8.024900\n",
      "Iteration 1240 => Loss: 8.021600\n",
      "Iteration 1241 => Loss: 8.018500\n",
      "Iteration 1242 => Loss: 8.015600\n",
      "Iteration 1243 => Loss: 8.012900\n",
      "Iteration 1244 => Loss: 8.010400\n",
      "Iteration 1245 => Loss: 8.008100\n",
      "Iteration 1246 => Loss: 8.006000\n",
      "Iteration 1247 => Loss: 8.004100\n",
      "Iteration 1248 => Loss: 8.003240\n",
      "Iteration 1249 => Loss: 7.998940\n",
      "Iteration 1250 => Loss: 7.994840\n",
      "Iteration 1251 => Loss: 7.990940\n",
      "Iteration 1252 => Loss: 7.987240\n",
      "Iteration 1253 => Loss: 7.983740\n",
      "Iteration 1254 => Loss: 7.980440\n",
      "Iteration 1255 => Loss: 7.977340\n",
      "Iteration 1256 => Loss: 7.974440\n",
      "Iteration 1257 => Loss: 7.971740\n",
      "Iteration 1258 => Loss: 7.969240\n",
      "Iteration 1259 => Loss: 7.966940\n",
      "Iteration 1260 => Loss: 7.964840\n",
      "Iteration 1261 => Loss: 7.962940\n",
      "Iteration 1262 => Loss: 7.961240\n",
      "Iteration 1263 => Loss: 7.959740\n",
      "Iteration 1264 => Loss: 7.958440\n",
      "Iteration 1265 => Loss: 7.957340\n",
      "Iteration 1266 => Loss: 7.954960\n",
      "Iteration 1267 => Loss: 7.951460\n",
      "Iteration 1268 => Loss: 7.948160\n",
      "Iteration 1269 => Loss: 7.945060\n",
      "Iteration 1270 => Loss: 7.942160\n",
      "Iteration 1271 => Loss: 7.939460\n",
      "Iteration 1272 => Loss: 7.936960\n",
      "Iteration 1273 => Loss: 7.934660\n",
      "Iteration 1274 => Loss: 7.932560\n",
      "Iteration 1275 => Loss: 7.930660\n",
      "Iteration 1276 => Loss: 7.928960\n",
      "Iteration 1277 => Loss: 7.927460\n",
      "Iteration 1278 => Loss: 7.926160\n",
      "Iteration 1279 => Loss: 7.925060\n",
      "Iteration 1280 => Loss: 7.924160\n",
      "Iteration 1281 => Loss: 7.923460\n",
      "Iteration 1282 => Loss: 7.922960\n",
      "Iteration 1283 => Loss: 7.921660\n",
      "Iteration 1284 => Loss: 7.918760\n",
      "Iteration 1285 => Loss: 7.916060\n",
      "Iteration 1286 => Loss: 7.913560\n",
      "Iteration 1287 => Loss: 7.911260\n",
      "Iteration 1288 => Loss: 7.909160\n",
      "Iteration 1289 => Loss: 7.907260\n",
      "Iteration 1290 => Loss: 7.905560\n",
      "Iteration 1291 => Loss: 7.904060\n",
      "Iteration 1292 => Loss: 7.902760\n",
      "Iteration 1293 => Loss: 7.901660\n",
      "Iteration 1294 => Loss: 7.900760\n",
      "Iteration 1295 => Loss: 7.900060\n",
      "Iteration 1296 => Loss: 7.899560\n",
      "Iteration 1297 => Loss: 7.899260\n",
      "Iteration 1298 => Loss: 7.899160\n",
      "\n",
      "w=1.670, b=10.090\n",
      "Prediction: x=20 => y=43.49\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Import data\n",
    "    X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)\n",
    "    # Train system\n",
    "    w, b = train(X, Y, iterations=10000, lr=0.01)\n",
    "    print(\"\\nw=%.3f, b=%.3f\" % (w, b))\n",
    "    print(\"Prediction: x=%d => y=%.2f\" % (20, predict(20, w, b)))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set() # activate Seaborn\n",
    "plt.axis([0, 50, 0, 50]) # scale axes (0 to 50)\n",
    "plt.xlabel(\"Reservations\", fontsize=20) # set x axis label\n",
    "plt.ylabel(\"Pizzas\", fontsize=20) # set y axis label\n",
    "#X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True) # load data\n",
    "ax = sns.regplot(x=X, y=Y, color=\"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
