{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict(X,w,b):return X * w + b\n",
    "def loss(X,Y,w,b):return np.average((predict(X,w,b) - Y) ** 2)\n",
    "def train(X,Y,iterations,lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        current_loss = loss(X,Y,w,b)\n",
    "        print(\"Iteration %4d => Loss: %.6f\"% (i,current_loss))\n",
    "        if loss(X,Y,w + lr,b) < current_loss:\n",
    "            w += lr\n",
    "        elif loss(X,Y,w - lr,b) < current_loss:\n",
    "            w -= lr\n",
    "        elif loss(X,Y,w,b + lr) < current_loss:\n",
    "            b += lr\n",
    "        elif loss(X,Y,w,b - lr) < current_loss:\n",
    "            b -= lr\n",
    "        else:\n",
    "            return w,b\n",
    "    raise Exception(\"Couldn't converge within %d iterations\" % iterations)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 1139.800000\n",
      "Iteration    1 => Loss: 1130.089340\n",
      "Iteration    2 => Loss: 1120.421360\n",
      "Iteration    3 => Loss: 1110.796060\n",
      "Iteration    4 => Loss: 1101.213440\n",
      "Iteration    5 => Loss: 1091.673500\n",
      "Iteration    6 => Loss: 1082.176240\n",
      "Iteration    7 => Loss: 1072.721660\n",
      "Iteration    8 => Loss: 1063.309760\n",
      "Iteration    9 => Loss: 1053.940540\n",
      "Iteration   10 => Loss: 1044.614000\n",
      "Iteration   11 => Loss: 1035.330140\n",
      "Iteration   12 => Loss: 1026.088960\n",
      "Iteration   13 => Loss: 1016.890460\n",
      "Iteration   14 => Loss: 1007.734640\n",
      "Iteration   15 => Loss: 998.621500\n",
      "Iteration   16 => Loss: 989.551040\n",
      "Iteration   17 => Loss: 980.523260\n",
      "Iteration   18 => Loss: 971.538160\n",
      "Iteration   19 => Loss: 962.595740\n",
      "Iteration   20 => Loss: 953.696000\n",
      "Iteration   21 => Loss: 944.838940\n",
      "Iteration   22 => Loss: 936.024560\n",
      "Iteration   23 => Loss: 927.252860\n",
      "Iteration   24 => Loss: 918.523840\n",
      "Iteration   25 => Loss: 909.837500\n",
      "Iteration   26 => Loss: 901.193840\n",
      "Iteration   27 => Loss: 892.592860\n",
      "Iteration   28 => Loss: 884.034560\n",
      "Iteration   29 => Loss: 875.518940\n",
      "Iteration   30 => Loss: 867.046000\n",
      "Iteration   31 => Loss: 858.615740\n",
      "Iteration   32 => Loss: 850.228160\n",
      "Iteration   33 => Loss: 841.883260\n",
      "Iteration   34 => Loss: 833.581040\n",
      "Iteration   35 => Loss: 825.321500\n",
      "Iteration   36 => Loss: 817.104640\n",
      "Iteration   37 => Loss: 808.930460\n",
      "Iteration   38 => Loss: 800.798960\n",
      "Iteration   39 => Loss: 792.710140\n",
      "Iteration   40 => Loss: 784.664000\n",
      "Iteration   41 => Loss: 776.660540\n",
      "Iteration   42 => Loss: 768.699760\n",
      "Iteration   43 => Loss: 760.781660\n",
      "Iteration   44 => Loss: 752.906240\n",
      "Iteration   45 => Loss: 745.073500\n",
      "Iteration   46 => Loss: 737.283440\n",
      "Iteration   47 => Loss: 729.536060\n",
      "Iteration   48 => Loss: 721.831360\n",
      "Iteration   49 => Loss: 714.169340\n",
      "Iteration   50 => Loss: 706.550000\n",
      "Iteration   51 => Loss: 698.973340\n",
      "Iteration   52 => Loss: 691.439360\n",
      "Iteration   53 => Loss: 683.948060\n",
      "Iteration   54 => Loss: 676.499440\n",
      "Iteration   55 => Loss: 669.093500\n",
      "Iteration   56 => Loss: 661.730240\n",
      "Iteration   57 => Loss: 654.409660\n",
      "Iteration   58 => Loss: 647.131760\n",
      "Iteration   59 => Loss: 639.896540\n",
      "Iteration   60 => Loss: 632.704000\n",
      "Iteration   61 => Loss: 625.554140\n",
      "Iteration   62 => Loss: 618.446960\n",
      "Iteration   63 => Loss: 611.382460\n",
      "Iteration   64 => Loss: 604.360640\n",
      "Iteration   65 => Loss: 597.381500\n",
      "Iteration   66 => Loss: 590.445040\n",
      "Iteration   67 => Loss: 583.551260\n",
      "Iteration   68 => Loss: 576.700160\n",
      "Iteration   69 => Loss: 569.891740\n",
      "Iteration   70 => Loss: 563.126000\n",
      "Iteration   71 => Loss: 556.402940\n",
      "Iteration   72 => Loss: 549.722560\n",
      "Iteration   73 => Loss: 543.084860\n",
      "Iteration   74 => Loss: 536.489840\n",
      "Iteration   75 => Loss: 529.937500\n",
      "Iteration   76 => Loss: 523.427840\n",
      "Iteration   77 => Loss: 516.960860\n",
      "Iteration   78 => Loss: 510.536560\n",
      "Iteration   79 => Loss: 504.154940\n",
      "Iteration   80 => Loss: 497.816000\n",
      "Iteration   81 => Loss: 491.519740\n",
      "Iteration   82 => Loss: 485.266160\n",
      "Iteration   83 => Loss: 479.055260\n",
      "Iteration   84 => Loss: 472.887040\n",
      "Iteration   85 => Loss: 466.761500\n",
      "Iteration   86 => Loss: 460.678640\n",
      "Iteration   87 => Loss: 454.638460\n",
      "Iteration   88 => Loss: 448.640960\n",
      "Iteration   89 => Loss: 442.686140\n",
      "Iteration   90 => Loss: 436.774000\n",
      "Iteration   91 => Loss: 430.904540\n",
      "Iteration   92 => Loss: 425.077760\n",
      "Iteration   93 => Loss: 419.293660\n",
      "Iteration   94 => Loss: 413.552240\n",
      "Iteration   95 => Loss: 407.853500\n",
      "Iteration   96 => Loss: 402.197440\n",
      "Iteration   97 => Loss: 396.584060\n",
      "Iteration   98 => Loss: 391.013360\n",
      "Iteration   99 => Loss: 385.485340\n",
      "Iteration  100 => Loss: 380.000000\n",
      "Iteration  101 => Loss: 374.557340\n",
      "Iteration  102 => Loss: 369.157360\n",
      "Iteration  103 => Loss: 363.800060\n",
      "Iteration  104 => Loss: 358.485440\n",
      "Iteration  105 => Loss: 353.213500\n",
      "Iteration  106 => Loss: 347.984240\n",
      "Iteration  107 => Loss: 342.797660\n",
      "Iteration  108 => Loss: 337.653760\n",
      "Iteration  109 => Loss: 332.552540\n",
      "Iteration  110 => Loss: 327.494000\n",
      "Iteration  111 => Loss: 322.478140\n",
      "Iteration  112 => Loss: 317.504960\n",
      "Iteration  113 => Loss: 312.574460\n",
      "Iteration  114 => Loss: 307.686640\n",
      "Iteration  115 => Loss: 302.841500\n",
      "Iteration  116 => Loss: 298.039040\n",
      "Iteration  117 => Loss: 293.279260\n",
      "Iteration  118 => Loss: 288.562160\n",
      "Iteration  119 => Loss: 283.887740\n",
      "Iteration  120 => Loss: 279.256000\n",
      "Iteration  121 => Loss: 274.666940\n",
      "Iteration  122 => Loss: 270.120560\n",
      "Iteration  123 => Loss: 265.616860\n",
      "Iteration  124 => Loss: 261.155840\n",
      "Iteration  125 => Loss: 256.737500\n",
      "Iteration  126 => Loss: 252.361840\n",
      "Iteration  127 => Loss: 248.028860\n",
      "Iteration  128 => Loss: 243.738560\n",
      "Iteration  129 => Loss: 239.490940\n",
      "Iteration  130 => Loss: 235.286000\n",
      "Iteration  131 => Loss: 231.123740\n",
      "Iteration  132 => Loss: 227.004160\n",
      "Iteration  133 => Loss: 222.927260\n",
      "Iteration  134 => Loss: 218.893040\n",
      "Iteration  135 => Loss: 214.901500\n",
      "Iteration  136 => Loss: 210.952640\n",
      "Iteration  137 => Loss: 207.046460\n",
      "Iteration  138 => Loss: 203.182960\n",
      "Iteration  139 => Loss: 199.362140\n",
      "Iteration  140 => Loss: 195.584000\n",
      "Iteration  141 => Loss: 191.848540\n",
      "Iteration  142 => Loss: 188.155760\n",
      "Iteration  143 => Loss: 184.505660\n",
      "Iteration  144 => Loss: 180.898240\n",
      "Iteration  145 => Loss: 177.333500\n",
      "Iteration  146 => Loss: 173.811440\n",
      "Iteration  147 => Loss: 170.332060\n",
      "Iteration  148 => Loss: 166.895360\n",
      "Iteration  149 => Loss: 163.501340\n",
      "Iteration  150 => Loss: 160.150000\n",
      "Iteration  151 => Loss: 156.841340\n",
      "Iteration  152 => Loss: 153.575360\n",
      "Iteration  153 => Loss: 150.352060\n",
      "Iteration  154 => Loss: 147.171440\n",
      "Iteration  155 => Loss: 144.033500\n",
      "Iteration  156 => Loss: 140.938240\n",
      "Iteration  157 => Loss: 137.885660\n",
      "Iteration  158 => Loss: 134.875760\n",
      "Iteration  159 => Loss: 131.908540\n",
      "Iteration  160 => Loss: 128.984000\n",
      "Iteration  161 => Loss: 126.102140\n",
      "Iteration  162 => Loss: 123.262960\n",
      "Iteration  163 => Loss: 120.466460\n",
      "Iteration  164 => Loss: 117.712640\n",
      "Iteration  165 => Loss: 115.001500\n",
      "Iteration  166 => Loss: 112.333040\n",
      "Iteration  167 => Loss: 109.707260\n",
      "Iteration  168 => Loss: 107.124160\n",
      "Iteration  169 => Loss: 104.583740\n",
      "Iteration  170 => Loss: 102.086000\n",
      "Iteration  171 => Loss: 99.630940\n",
      "Iteration  172 => Loss: 97.218560\n",
      "Iteration  173 => Loss: 94.848860\n",
      "Iteration  174 => Loss: 92.521840\n",
      "Iteration  175 => Loss: 90.237500\n",
      "Iteration  176 => Loss: 87.995840\n",
      "Iteration  177 => Loss: 85.796860\n",
      "Iteration  178 => Loss: 83.640560\n",
      "Iteration  179 => Loss: 81.526940\n",
      "Iteration  180 => Loss: 79.456000\n",
      "Iteration  181 => Loss: 77.427740\n",
      "Iteration  182 => Loss: 75.442160\n",
      "Iteration  183 => Loss: 73.499260\n",
      "Iteration  184 => Loss: 71.599040\n",
      "Iteration  185 => Loss: 69.741500\n",
      "Iteration  186 => Loss: 67.926640\n",
      "Iteration  187 => Loss: 66.154460\n",
      "Iteration  188 => Loss: 64.424960\n",
      "Iteration  189 => Loss: 62.738140\n",
      "Iteration  190 => Loss: 61.094000\n",
      "Iteration  191 => Loss: 59.492540\n",
      "Iteration  192 => Loss: 57.933760\n",
      "Iteration  193 => Loss: 56.417660\n",
      "Iteration  194 => Loss: 54.944240\n",
      "Iteration  195 => Loss: 53.513500\n",
      "Iteration  196 => Loss: 52.125440\n",
      "Iteration  197 => Loss: 50.780060\n",
      "Iteration  198 => Loss: 49.477360\n",
      "Iteration  199 => Loss: 48.217340\n",
      "Iteration  200 => Loss: 47.000000\n",
      "Iteration  201 => Loss: 45.825340\n",
      "Iteration  202 => Loss: 44.693360\n",
      "Iteration  203 => Loss: 43.604060\n",
      "Iteration  204 => Loss: 42.557440\n",
      "Iteration  205 => Loss: 41.553500\n",
      "Iteration  206 => Loss: 40.592240\n",
      "Iteration  207 => Loss: 39.673660\n",
      "Iteration  208 => Loss: 38.797760\n",
      "Iteration  209 => Loss: 37.964540\n",
      "Iteration  210 => Loss: 37.174000\n",
      "Iteration  211 => Loss: 36.426140\n",
      "Iteration  212 => Loss: 35.720960\n",
      "Iteration  213 => Loss: 35.058460\n",
      "Iteration  214 => Loss: 34.438640\n",
      "Iteration  215 => Loss: 33.861500\n",
      "Iteration  216 => Loss: 33.327040\n",
      "Iteration  217 => Loss: 32.835260\n",
      "Iteration  218 => Loss: 32.386160\n",
      "Iteration  219 => Loss: 31.979740\n",
      "Iteration  220 => Loss: 31.616000\n",
      "Iteration  221 => Loss: 31.294940\n",
      "Iteration  222 => Loss: 31.016560\n",
      "Iteration  223 => Loss: 30.780860\n",
      "Iteration  224 => Loss: 30.587840\n",
      "Iteration  225 => Loss: 30.437500\n",
      "Iteration  226 => Loss: 30.329840\n",
      "Iteration  227 => Loss: 30.264860\n",
      "Iteration  228 => Loss: 30.242560\n",
      "Iteration  229 => Loss: 30.199460\n",
      "Iteration  230 => Loss: 30.156560\n",
      "Iteration  231 => Loss: 30.113860\n",
      "Iteration  232 => Loss: 30.071360\n",
      "Iteration  233 => Loss: 30.029060\n",
      "Iteration  234 => Loss: 29.986960\n",
      "Iteration  235 => Loss: 29.945060\n",
      "Iteration  236 => Loss: 29.903360\n",
      "Iteration  237 => Loss: 29.861860\n",
      "Iteration  238 => Loss: 29.860760\n",
      "Iteration  239 => Loss: 29.816860\n",
      "Iteration  240 => Loss: 29.773160\n",
      "Iteration  241 => Loss: 29.729660\n",
      "Iteration  242 => Loss: 29.686360\n",
      "Iteration  243 => Loss: 29.643260\n",
      "Iteration  244 => Loss: 29.600360\n",
      "Iteration  245 => Loss: 29.557660\n",
      "Iteration  246 => Loss: 29.515160\n",
      "Iteration  247 => Loss: 29.472860\n",
      "Iteration  248 => Loss: 29.430760\n",
      "Iteration  249 => Loss: 29.388860\n",
      "Iteration  250 => Loss: 29.347160\n",
      "Iteration  251 => Loss: 29.305660\n",
      "Iteration  252 => Loss: 29.264360\n",
      "Iteration  253 => Loss: 29.223260\n",
      "Iteration  254 => Loss: 29.182360\n",
      "Iteration  255 => Loss: 29.182340\n",
      "Iteration  256 => Loss: 29.139040\n",
      "Iteration  257 => Loss: 29.095940\n",
      "Iteration  258 => Loss: 29.053040\n",
      "Iteration  259 => Loss: 29.010340\n",
      "Iteration  260 => Loss: 28.967840\n",
      "Iteration  261 => Loss: 28.925540\n",
      "Iteration  262 => Loss: 28.883440\n",
      "Iteration  263 => Loss: 28.841540\n",
      "Iteration  264 => Loss: 28.799840\n",
      "Iteration  265 => Loss: 28.758340\n",
      "Iteration  266 => Loss: 28.717040\n",
      "Iteration  267 => Loss: 28.675940\n",
      "Iteration  268 => Loss: 28.635040\n",
      "Iteration  269 => Loss: 28.594340\n",
      "Iteration  270 => Loss: 28.553840\n",
      "Iteration  271 => Loss: 28.513540\n",
      "Iteration  272 => Loss: 28.473440\n",
      "Iteration  273 => Loss: 28.471900\n",
      "Iteration  274 => Loss: 28.429400\n",
      "Iteration  275 => Loss: 28.387100\n",
      "Iteration  276 => Loss: 28.345000\n",
      "Iteration  277 => Loss: 28.303100\n",
      "Iteration  278 => Loss: 28.261400\n",
      "Iteration  279 => Loss: 28.219900\n",
      "Iteration  280 => Loss: 28.178600\n",
      "Iteration  281 => Loss: 28.137500\n",
      "Iteration  282 => Loss: 28.096600\n",
      "Iteration  283 => Loss: 28.055900\n",
      "Iteration  284 => Loss: 28.015400\n",
      "Iteration  285 => Loss: 27.975100\n",
      "Iteration  286 => Loss: 27.935000\n",
      "Iteration  287 => Loss: 27.895100\n",
      "Iteration  288 => Loss: 27.855400\n",
      "Iteration  289 => Loss: 27.815900\n",
      "Iteration  290 => Loss: 27.815440\n",
      "Iteration  291 => Loss: 27.773540\n",
      "Iteration  292 => Loss: 27.731840\n",
      "Iteration  293 => Loss: 27.690340\n",
      "Iteration  294 => Loss: 27.649040\n",
      "Iteration  295 => Loss: 27.607940\n",
      "Iteration  296 => Loss: 27.567040\n",
      "Iteration  297 => Loss: 27.526340\n",
      "Iteration  298 => Loss: 27.485840\n",
      "Iteration  299 => Loss: 27.445540\n",
      "Iteration  300 => Loss: 27.405440\n",
      "Iteration  301 => Loss: 27.365540\n",
      "Iteration  302 => Loss: 27.325840\n",
      "Iteration  303 => Loss: 27.286340\n",
      "Iteration  304 => Loss: 27.247040\n",
      "Iteration  305 => Loss: 27.207940\n",
      "Iteration  306 => Loss: 27.169040\n",
      "Iteration  307 => Loss: 27.130340\n",
      "Iteration  308 => Loss: 27.128360\n",
      "Iteration  309 => Loss: 27.087260\n",
      "Iteration  310 => Loss: 27.046360\n",
      "Iteration  311 => Loss: 27.005660\n",
      "Iteration  312 => Loss: 26.965160\n",
      "Iteration  313 => Loss: 26.924860\n",
      "Iteration  314 => Loss: 26.884760\n",
      "Iteration  315 => Loss: 26.844860\n",
      "Iteration  316 => Loss: 26.805160\n",
      "Iteration  317 => Loss: 26.765660\n",
      "Iteration  318 => Loss: 26.726360\n",
      "Iteration  319 => Loss: 26.687260\n",
      "Iteration  320 => Loss: 26.648360\n",
      "Iteration  321 => Loss: 26.609660\n",
      "Iteration  322 => Loss: 26.571160\n",
      "Iteration  323 => Loss: 26.532860\n",
      "Iteration  324 => Loss: 26.494760\n",
      "Iteration  325 => Loss: 26.493860\n",
      "Iteration  326 => Loss: 26.453360\n",
      "Iteration  327 => Loss: 26.413060\n",
      "Iteration  328 => Loss: 26.372960\n",
      "Iteration  329 => Loss: 26.333060\n",
      "Iteration  330 => Loss: 26.293360\n",
      "Iteration  331 => Loss: 26.253860\n",
      "Iteration  332 => Loss: 26.214560\n",
      "Iteration  333 => Loss: 26.175460\n",
      "Iteration  334 => Loss: 26.136560\n",
      "Iteration  335 => Loss: 26.097860\n",
      "Iteration  336 => Loss: 26.059360\n",
      "Iteration  337 => Loss: 26.021060\n",
      "Iteration  338 => Loss: 25.982960\n",
      "Iteration  339 => Loss: 25.945060\n",
      "Iteration  340 => Loss: 25.907360\n",
      "Iteration  341 => Loss: 25.869860\n",
      "Iteration  342 => Loss: 25.832560\n",
      "Iteration  343 => Loss: 25.830140\n",
      "Iteration  344 => Loss: 25.790440\n",
      "Iteration  345 => Loss: 25.750940\n",
      "Iteration  346 => Loss: 25.711640\n",
      "Iteration  347 => Loss: 25.672540\n",
      "Iteration  348 => Loss: 25.633640\n",
      "Iteration  349 => Loss: 25.594940\n",
      "Iteration  350 => Loss: 25.556440\n",
      "Iteration  351 => Loss: 25.518140\n",
      "Iteration  352 => Loss: 25.480040\n",
      "Iteration  353 => Loss: 25.442140\n",
      "Iteration  354 => Loss: 25.404440\n",
      "Iteration  355 => Loss: 25.366940\n",
      "Iteration  356 => Loss: 25.329640\n",
      "Iteration  357 => Loss: 25.292540\n",
      "Iteration  358 => Loss: 25.255640\n",
      "Iteration  359 => Loss: 25.218940\n",
      "Iteration  360 => Loss: 25.217600\n",
      "Iteration  361 => Loss: 25.178500\n",
      "Iteration  362 => Loss: 25.139600\n",
      "Iteration  363 => Loss: 25.100900\n",
      "Iteration  364 => Loss: 25.062400\n",
      "Iteration  365 => Loss: 25.024100\n",
      "Iteration  366 => Loss: 24.986000\n",
      "Iteration  367 => Loss: 24.948100\n",
      "Iteration  368 => Loss: 24.910400\n",
      "Iteration  369 => Loss: 24.872900\n",
      "Iteration  370 => Loss: 24.835600\n",
      "Iteration  371 => Loss: 24.798500\n",
      "Iteration  372 => Loss: 24.761600\n",
      "Iteration  373 => Loss: 24.724900\n",
      "Iteration  374 => Loss: 24.688400\n",
      "Iteration  375 => Loss: 24.652100\n",
      "Iteration  376 => Loss: 24.616000\n",
      "Iteration  377 => Loss: 24.615740\n",
      "Iteration  378 => Loss: 24.577240\n",
      "Iteration  379 => Loss: 24.538940\n",
      "Iteration  380 => Loss: 24.500840\n",
      "Iteration  381 => Loss: 24.462940\n",
      "Iteration  382 => Loss: 24.425240\n",
      "Iteration  383 => Loss: 24.387740\n",
      "Iteration  384 => Loss: 24.350440\n",
      "Iteration  385 => Loss: 24.313340\n",
      "Iteration  386 => Loss: 24.276440\n",
      "Iteration  387 => Loss: 24.239740\n",
      "Iteration  388 => Loss: 24.203240\n",
      "Iteration  389 => Loss: 24.166940\n",
      "Iteration  390 => Loss: 24.130840\n",
      "Iteration  391 => Loss: 24.094940\n",
      "Iteration  392 => Loss: 24.059240\n",
      "Iteration  393 => Loss: 24.023740\n",
      "Iteration  394 => Loss: 23.988440\n",
      "Iteration  395 => Loss: 23.986660\n",
      "Iteration  396 => Loss: 23.948960\n",
      "Iteration  397 => Loss: 23.911460\n",
      "Iteration  398 => Loss: 23.874160\n",
      "Iteration  399 => Loss: 23.837060\n",
      "Iteration  400 => Loss: 23.800160\n",
      "Iteration  401 => Loss: 23.763460\n",
      "Iteration  402 => Loss: 23.726960\n",
      "Iteration  403 => Loss: 23.690660\n",
      "Iteration  404 => Loss: 23.654560\n",
      "Iteration  405 => Loss: 23.618660\n",
      "Iteration  406 => Loss: 23.582960\n",
      "Iteration  407 => Loss: 23.547460\n",
      "Iteration  408 => Loss: 23.512160\n",
      "Iteration  409 => Loss: 23.477060\n",
      "Iteration  410 => Loss: 23.442160\n",
      "Iteration  411 => Loss: 23.407460\n",
      "Iteration  412 => Loss: 23.406760\n",
      "Iteration  413 => Loss: 23.369660\n",
      "Iteration  414 => Loss: 23.332760\n",
      "Iteration  415 => Loss: 23.296060\n",
      "Iteration  416 => Loss: 23.259560\n",
      "Iteration  417 => Loss: 23.223260\n",
      "Iteration  418 => Loss: 23.187160\n",
      "Iteration  419 => Loss: 23.151260\n",
      "Iteration  420 => Loss: 23.115560\n",
      "Iteration  421 => Loss: 23.080060\n",
      "Iteration  422 => Loss: 23.044760\n",
      "Iteration  423 => Loss: 23.009660\n",
      "Iteration  424 => Loss: 22.974760\n",
      "Iteration  425 => Loss: 22.940060\n",
      "Iteration  426 => Loss: 22.905560\n",
      "Iteration  427 => Loss: 22.871260\n",
      "Iteration  428 => Loss: 22.837160\n",
      "Iteration  429 => Loss: 22.803260\n",
      "Iteration  430 => Loss: 22.801040\n",
      "Iteration  431 => Loss: 22.764740\n",
      "Iteration  432 => Loss: 22.728640\n",
      "Iteration  433 => Loss: 22.692740\n",
      "Iteration  434 => Loss: 22.657040\n",
      "Iteration  435 => Loss: 22.621540\n",
      "Iteration  436 => Loss: 22.586240\n",
      "Iteration  437 => Loss: 22.551140\n",
      "Iteration  438 => Loss: 22.516240\n",
      "Iteration  439 => Loss: 22.481540\n",
      "Iteration  440 => Loss: 22.447040\n",
      "Iteration  441 => Loss: 22.412740\n",
      "Iteration  442 => Loss: 22.378640\n",
      "Iteration  443 => Loss: 22.344740\n",
      "Iteration  444 => Loss: 22.311040\n",
      "Iteration  445 => Loss: 22.277540\n",
      "Iteration  446 => Loss: 22.244240\n",
      "Iteration  447 => Loss: 22.243100\n",
      "Iteration  448 => Loss: 22.207400\n",
      "Iteration  449 => Loss: 22.171900\n",
      "Iteration  450 => Loss: 22.136600\n",
      "Iteration  451 => Loss: 22.101500\n",
      "Iteration  452 => Loss: 22.066600\n",
      "Iteration  453 => Loss: 22.031900\n",
      "Iteration  454 => Loss: 21.997400\n",
      "Iteration  455 => Loss: 21.963100\n",
      "Iteration  456 => Loss: 21.929000\n",
      "Iteration  457 => Loss: 21.895100\n",
      "Iteration  458 => Loss: 21.861400\n",
      "Iteration  459 => Loss: 21.827900\n",
      "Iteration  460 => Loss: 21.794600\n",
      "Iteration  461 => Loss: 21.761500\n",
      "Iteration  462 => Loss: 21.728600\n",
      "Iteration  463 => Loss: 21.695900\n",
      "Iteration  464 => Loss: 21.695840\n",
      "Iteration  465 => Loss: 21.660740\n",
      "Iteration  466 => Loss: 21.625840\n",
      "Iteration  467 => Loss: 21.591140\n",
      "Iteration  468 => Loss: 21.556640\n",
      "Iteration  469 => Loss: 21.522340\n",
      "Iteration  470 => Loss: 21.488240\n",
      "Iteration  471 => Loss: 21.454340\n",
      "Iteration  472 => Loss: 21.420640\n",
      "Iteration  473 => Loss: 21.387140\n",
      "Iteration  474 => Loss: 21.353840\n",
      "Iteration  475 => Loss: 21.320740\n",
      "Iteration  476 => Loss: 21.287840\n",
      "Iteration  477 => Loss: 21.255140\n",
      "Iteration  478 => Loss: 21.222640\n",
      "Iteration  479 => Loss: 21.190340\n",
      "Iteration  480 => Loss: 21.158240\n",
      "Iteration  481 => Loss: 21.126340\n",
      "Iteration  482 => Loss: 21.124760\n",
      "Iteration  483 => Loss: 21.090460\n",
      "Iteration  484 => Loss: 21.056360\n",
      "Iteration  485 => Loss: 21.022460\n",
      "Iteration  486 => Loss: 20.988760\n",
      "Iteration  487 => Loss: 20.955260\n",
      "Iteration  488 => Loss: 20.921960\n",
      "Iteration  489 => Loss: 20.888860\n",
      "Iteration  490 => Loss: 20.855960\n",
      "Iteration  491 => Loss: 20.823260\n",
      "Iteration  492 => Loss: 20.790760\n",
      "Iteration  493 => Loss: 20.758460\n",
      "Iteration  494 => Loss: 20.726360\n",
      "Iteration  495 => Loss: 20.694460\n",
      "Iteration  496 => Loss: 20.662760\n",
      "Iteration  497 => Loss: 20.631260\n",
      "Iteration  498 => Loss: 20.599960\n",
      "Iteration  499 => Loss: 20.599460\n",
      "Iteration  500 => Loss: 20.565760\n",
      "Iteration  501 => Loss: 20.532260\n",
      "Iteration  502 => Loss: 20.498960\n",
      "Iteration  503 => Loss: 20.465860\n",
      "Iteration  504 => Loss: 20.432960\n",
      "Iteration  505 => Loss: 20.400260\n",
      "Iteration  506 => Loss: 20.367760\n",
      "Iteration  507 => Loss: 20.335460\n",
      "Iteration  508 => Loss: 20.303360\n",
      "Iteration  509 => Loss: 20.271460\n",
      "Iteration  510 => Loss: 20.239760\n",
      "Iteration  511 => Loss: 20.208260\n",
      "Iteration  512 => Loss: 20.176960\n",
      "Iteration  513 => Loss: 20.145860\n",
      "Iteration  514 => Loss: 20.114960\n",
      "Iteration  515 => Loss: 20.084260\n",
      "Iteration  516 => Loss: 20.053760\n",
      "Iteration  517 => Loss: 20.051740\n",
      "Iteration  518 => Loss: 20.018840\n",
      "Iteration  519 => Loss: 19.986140\n",
      "Iteration  520 => Loss: 19.953640\n",
      "Iteration  521 => Loss: 19.921340\n",
      "Iteration  522 => Loss: 19.889240\n",
      "Iteration  523 => Loss: 19.857340\n",
      "Iteration  524 => Loss: 19.825640\n",
      "Iteration  525 => Loss: 19.794140\n",
      "Iteration  526 => Loss: 19.762840\n",
      "Iteration  527 => Loss: 19.731740\n",
      "Iteration  528 => Loss: 19.700840\n",
      "Iteration  529 => Loss: 19.670140\n",
      "Iteration  530 => Loss: 19.639640\n",
      "Iteration  531 => Loss: 19.609340\n",
      "Iteration  532 => Loss: 19.579240\n",
      "Iteration  533 => Loss: 19.549340\n",
      "Iteration  534 => Loss: 19.548400\n",
      "Iteration  535 => Loss: 19.516100\n",
      "Iteration  536 => Loss: 19.484000\n",
      "Iteration  537 => Loss: 19.452100\n",
      "Iteration  538 => Loss: 19.420400\n",
      "Iteration  539 => Loss: 19.388900\n",
      "Iteration  540 => Loss: 19.357600\n",
      "Iteration  541 => Loss: 19.326500\n",
      "Iteration  542 => Loss: 19.295600\n",
      "Iteration  543 => Loss: 19.264900\n",
      "Iteration  544 => Loss: 19.234400\n",
      "Iteration  545 => Loss: 19.204100\n",
      "Iteration  546 => Loss: 19.174000\n",
      "Iteration  547 => Loss: 19.144100\n",
      "Iteration  548 => Loss: 19.114400\n",
      "Iteration  549 => Loss: 19.084900\n",
      "Iteration  550 => Loss: 19.055600\n",
      "Iteration  551 => Loss: 19.026500\n",
      "Iteration  552 => Loss: 19.024040\n",
      "Iteration  553 => Loss: 18.992540\n",
      "Iteration  554 => Loss: 18.961240\n",
      "Iteration  555 => Loss: 18.930140\n",
      "Iteration  556 => Loss: 18.899240\n",
      "Iteration  557 => Loss: 18.868540\n",
      "Iteration  558 => Loss: 18.838040\n",
      "Iteration  559 => Loss: 18.807740\n",
      "Iteration  560 => Loss: 18.777640\n",
      "Iteration  561 => Loss: 18.747740\n",
      "Iteration  562 => Loss: 18.718040\n",
      "Iteration  563 => Loss: 18.688540\n",
      "Iteration  564 => Loss: 18.659240\n",
      "Iteration  565 => Loss: 18.630140\n",
      "Iteration  566 => Loss: 18.601240\n",
      "Iteration  567 => Loss: 18.572540\n",
      "Iteration  568 => Loss: 18.544040\n",
      "Iteration  569 => Loss: 18.542660\n",
      "Iteration  570 => Loss: 18.511760\n",
      "Iteration  571 => Loss: 18.481060\n",
      "Iteration  572 => Loss: 18.450560\n",
      "Iteration  573 => Loss: 18.420260\n",
      "Iteration  574 => Loss: 18.390160\n",
      "Iteration  575 => Loss: 18.360260\n",
      "Iteration  576 => Loss: 18.330560\n",
      "Iteration  577 => Loss: 18.301060\n",
      "Iteration  578 => Loss: 18.271760\n",
      "Iteration  579 => Loss: 18.242660\n",
      "Iteration  580 => Loss: 18.213760\n",
      "Iteration  581 => Loss: 18.185060\n",
      "Iteration  582 => Loss: 18.156560\n",
      "Iteration  583 => Loss: 18.128260\n",
      "Iteration  584 => Loss: 18.100160\n",
      "Iteration  585 => Loss: 18.072260\n",
      "Iteration  586 => Loss: 18.071960\n",
      "Iteration  587 => Loss: 18.041660\n",
      "Iteration  588 => Loss: 18.011560\n",
      "Iteration  589 => Loss: 17.981660\n",
      "Iteration  590 => Loss: 17.951960\n",
      "Iteration  591 => Loss: 17.922460\n",
      "Iteration  592 => Loss: 17.893160\n",
      "Iteration  593 => Loss: 17.864060\n",
      "Iteration  594 => Loss: 17.835160\n",
      "Iteration  595 => Loss: 17.806460\n",
      "Iteration  596 => Loss: 17.777960\n",
      "Iteration  597 => Loss: 17.749660\n",
      "Iteration  598 => Loss: 17.721560\n",
      "Iteration  599 => Loss: 17.693660\n",
      "Iteration  600 => Loss: 17.665960\n",
      "Iteration  601 => Loss: 17.638460\n",
      "Iteration  602 => Loss: 17.611160\n",
      "Iteration  603 => Loss: 17.584060\n",
      "Iteration  604 => Loss: 17.582240\n",
      "Iteration  605 => Loss: 17.552740\n",
      "Iteration  606 => Loss: 17.523440\n",
      "Iteration  607 => Loss: 17.494340\n",
      "Iteration  608 => Loss: 17.465440\n",
      "Iteration  609 => Loss: 17.436740\n",
      "Iteration  610 => Loss: 17.408240\n",
      "Iteration  611 => Loss: 17.379940\n",
      "Iteration  612 => Loss: 17.351840\n",
      "Iteration  613 => Loss: 17.323940\n",
      "Iteration  614 => Loss: 17.296240\n",
      "Iteration  615 => Loss: 17.268740\n",
      "Iteration  616 => Loss: 17.241440\n",
      "Iteration  617 => Loss: 17.214340\n",
      "Iteration  618 => Loss: 17.187440\n",
      "Iteration  619 => Loss: 17.160740\n",
      "Iteration  620 => Loss: 17.134240\n",
      "Iteration  621 => Loss: 17.133500\n",
      "Iteration  622 => Loss: 17.104600\n",
      "Iteration  623 => Loss: 17.075900\n",
      "Iteration  624 => Loss: 17.047400\n",
      "Iteration  625 => Loss: 17.019100\n",
      "Iteration  626 => Loss: 16.991000\n",
      "Iteration  627 => Loss: 16.963100\n",
      "Iteration  628 => Loss: 16.935400\n",
      "Iteration  629 => Loss: 16.907900\n",
      "Iteration  630 => Loss: 16.880600\n",
      "Iteration  631 => Loss: 16.853500\n",
      "Iteration  632 => Loss: 16.826600\n",
      "Iteration  633 => Loss: 16.799900\n",
      "Iteration  634 => Loss: 16.773400\n",
      "Iteration  635 => Loss: 16.747100\n",
      "Iteration  636 => Loss: 16.721000\n",
      "Iteration  637 => Loss: 16.695100\n",
      "Iteration  638 => Loss: 16.669400\n",
      "Iteration  639 => Loss: 16.667140\n",
      "Iteration  640 => Loss: 16.639040\n",
      "Iteration  641 => Loss: 16.611140\n",
      "Iteration  642 => Loss: 16.583440\n",
      "Iteration  643 => Loss: 16.555940\n",
      "Iteration  644 => Loss: 16.528640\n",
      "Iteration  645 => Loss: 16.501540\n",
      "Iteration  646 => Loss: 16.474640\n",
      "Iteration  647 => Loss: 16.447940\n",
      "Iteration  648 => Loss: 16.421440\n",
      "Iteration  649 => Loss: 16.395140\n",
      "Iteration  650 => Loss: 16.369040\n",
      "Iteration  651 => Loss: 16.343140\n",
      "Iteration  652 => Loss: 16.317440\n",
      "Iteration  653 => Loss: 16.291940\n",
      "Iteration  654 => Loss: 16.266640\n",
      "Iteration  655 => Loss: 16.241540\n",
      "Iteration  656 => Loss: 16.240360\n",
      "Iteration  657 => Loss: 16.212860\n",
      "Iteration  658 => Loss: 16.185560\n",
      "Iteration  659 => Loss: 16.158460\n",
      "Iteration  660 => Loss: 16.131560\n",
      "Iteration  661 => Loss: 16.104860\n",
      "Iteration  662 => Loss: 16.078360\n",
      "Iteration  663 => Loss: 16.052060\n",
      "Iteration  664 => Loss: 16.025960\n",
      "Iteration  665 => Loss: 16.000060\n",
      "Iteration  666 => Loss: 15.974360\n",
      "Iteration  667 => Loss: 15.948860\n",
      "Iteration  668 => Loss: 15.923560\n",
      "Iteration  669 => Loss: 15.898460\n",
      "Iteration  670 => Loss: 15.873560\n",
      "Iteration  671 => Loss: 15.848860\n",
      "Iteration  672 => Loss: 15.824360\n",
      "Iteration  673 => Loss: 15.824260\n",
      "Iteration  674 => Loss: 15.797360\n",
      "Iteration  675 => Loss: 15.770660\n",
      "Iteration  676 => Loss: 15.744160\n",
      "Iteration  677 => Loss: 15.717860\n",
      "Iteration  678 => Loss: 15.691760\n",
      "Iteration  679 => Loss: 15.665860\n",
      "Iteration  680 => Loss: 15.640160\n",
      "Iteration  681 => Loss: 15.614660\n",
      "Iteration  682 => Loss: 15.589360\n",
      "Iteration  683 => Loss: 15.564260\n",
      "Iteration  684 => Loss: 15.539360\n",
      "Iteration  685 => Loss: 15.514660\n",
      "Iteration  686 => Loss: 15.490160\n",
      "Iteration  687 => Loss: 15.465860\n",
      "Iteration  688 => Loss: 15.441760\n",
      "Iteration  689 => Loss: 15.417860\n",
      "Iteration  690 => Loss: 15.394160\n",
      "Iteration  691 => Loss: 15.392540\n",
      "Iteration  692 => Loss: 15.366440\n",
      "Iteration  693 => Loss: 15.340540\n",
      "Iteration  694 => Loss: 15.314840\n",
      "Iteration  695 => Loss: 15.289340\n",
      "Iteration  696 => Loss: 15.264040\n",
      "Iteration  697 => Loss: 15.238940\n",
      "Iteration  698 => Loss: 15.214040\n",
      "Iteration  699 => Loss: 15.189340\n",
      "Iteration  700 => Loss: 15.164840\n",
      "Iteration  701 => Loss: 15.140540\n",
      "Iteration  702 => Loss: 15.116440\n",
      "Iteration  703 => Loss: 15.092540\n",
      "Iteration  704 => Loss: 15.068840\n",
      "Iteration  705 => Loss: 15.045340\n",
      "Iteration  706 => Loss: 15.022040\n",
      "Iteration  707 => Loss: 14.998940\n",
      "Iteration  708 => Loss: 14.998400\n",
      "Iteration  709 => Loss: 14.972900\n",
      "Iteration  710 => Loss: 14.947600\n",
      "Iteration  711 => Loss: 14.922500\n",
      "Iteration  712 => Loss: 14.897600\n",
      "Iteration  713 => Loss: 14.872900\n",
      "Iteration  714 => Loss: 14.848400\n",
      "Iteration  715 => Loss: 14.824100\n",
      "Iteration  716 => Loss: 14.800000\n",
      "Iteration  717 => Loss: 14.776100\n",
      "Iteration  718 => Loss: 14.752400\n",
      "Iteration  719 => Loss: 14.728900\n",
      "Iteration  720 => Loss: 14.705600\n",
      "Iteration  721 => Loss: 14.682500\n",
      "Iteration  722 => Loss: 14.659600\n",
      "Iteration  723 => Loss: 14.636900\n",
      "Iteration  724 => Loss: 14.614400\n",
      "Iteration  725 => Loss: 14.592100\n",
      "Iteration  726 => Loss: 14.590040\n",
      "Iteration  727 => Loss: 14.565340\n",
      "Iteration  728 => Loss: 14.540840\n",
      "Iteration  729 => Loss: 14.516540\n",
      "Iteration  730 => Loss: 14.492440\n",
      "Iteration  731 => Loss: 14.468540\n",
      "Iteration  732 => Loss: 14.444840\n",
      "Iteration  733 => Loss: 14.421340\n",
      "Iteration  734 => Loss: 14.398040\n",
      "Iteration  735 => Loss: 14.374940\n",
      "Iteration  736 => Loss: 14.352040\n",
      "Iteration  737 => Loss: 14.329340\n",
      "Iteration  738 => Loss: 14.306840\n",
      "Iteration  739 => Loss: 14.284540\n",
      "Iteration  740 => Loss: 14.262440\n",
      "Iteration  741 => Loss: 14.240540\n",
      "Iteration  742 => Loss: 14.218840\n",
      "Iteration  743 => Loss: 14.217860\n",
      "Iteration  744 => Loss: 14.193760\n",
      "Iteration  745 => Loss: 14.169860\n",
      "Iteration  746 => Loss: 14.146160\n",
      "Iteration  747 => Loss: 14.122660\n",
      "Iteration  748 => Loss: 14.099360\n",
      "Iteration  749 => Loss: 14.076260\n",
      "Iteration  750 => Loss: 14.053360\n",
      "Iteration  751 => Loss: 14.030660\n",
      "Iteration  752 => Loss: 14.008160\n",
      "Iteration  753 => Loss: 13.985860\n",
      "Iteration  754 => Loss: 13.963760\n",
      "Iteration  755 => Loss: 13.941860\n",
      "Iteration  756 => Loss: 13.920160\n",
      "Iteration  757 => Loss: 13.898660\n",
      "Iteration  758 => Loss: 13.877360\n",
      "Iteration  759 => Loss: 13.856260\n",
      "Iteration  760 => Loss: 13.835360\n",
      "Iteration  761 => Loss: 13.832860\n",
      "Iteration  762 => Loss: 13.809560\n",
      "Iteration  763 => Loss: 13.786460\n",
      "Iteration  764 => Loss: 13.763560\n",
      "Iteration  765 => Loss: 13.740860\n",
      "Iteration  766 => Loss: 13.718360\n",
      "Iteration  767 => Loss: 13.696060\n",
      "Iteration  768 => Loss: 13.673960\n",
      "Iteration  769 => Loss: 13.652060\n",
      "Iteration  770 => Loss: 13.630360\n",
      "Iteration  771 => Loss: 13.608860\n",
      "Iteration  772 => Loss: 13.587560\n",
      "Iteration  773 => Loss: 13.566460\n",
      "Iteration  774 => Loss: 13.545560\n",
      "Iteration  775 => Loss: 13.524860\n",
      "Iteration  776 => Loss: 13.504360\n",
      "Iteration  777 => Loss: 13.484060\n",
      "Iteration  778 => Loss: 13.482640\n",
      "Iteration  779 => Loss: 13.459940\n",
      "Iteration  780 => Loss: 13.437440\n",
      "Iteration  781 => Loss: 13.415140\n",
      "Iteration  782 => Loss: 13.393040\n",
      "Iteration  783 => Loss: 13.371140\n",
      "Iteration  784 => Loss: 13.349440\n",
      "Iteration  785 => Loss: 13.327940\n",
      "Iteration  786 => Loss: 13.306640\n",
      "Iteration  787 => Loss: 13.285540\n",
      "Iteration  788 => Loss: 13.264640\n",
      "Iteration  789 => Loss: 13.243940\n",
      "Iteration  790 => Loss: 13.223440\n",
      "Iteration  791 => Loss: 13.203140\n",
      "Iteration  792 => Loss: 13.183040\n",
      "Iteration  793 => Loss: 13.163140\n",
      "Iteration  794 => Loss: 13.143440\n",
      "Iteration  795 => Loss: 13.143100\n",
      "Iteration  796 => Loss: 13.121000\n",
      "Iteration  797 => Loss: 13.099100\n",
      "Iteration  798 => Loss: 13.077400\n",
      "Iteration  799 => Loss: 13.055900\n",
      "Iteration  800 => Loss: 13.034600\n",
      "Iteration  801 => Loss: 13.013500\n",
      "Iteration  802 => Loss: 12.992600\n",
      "Iteration  803 => Loss: 12.971900\n",
      "Iteration  804 => Loss: 12.951400\n",
      "Iteration  805 => Loss: 12.931100\n",
      "Iteration  806 => Loss: 12.911000\n",
      "Iteration  807 => Loss: 12.891100\n",
      "Iteration  808 => Loss: 12.871400\n",
      "Iteration  809 => Loss: 12.851900\n",
      "Iteration  810 => Loss: 12.832600\n",
      "Iteration  811 => Loss: 12.813500\n",
      "Iteration  812 => Loss: 12.794600\n",
      "Iteration  813 => Loss: 12.792740\n",
      "Iteration  814 => Loss: 12.771440\n",
      "Iteration  815 => Loss: 12.750340\n",
      "Iteration  816 => Loss: 12.729440\n",
      "Iteration  817 => Loss: 12.708740\n",
      "Iteration  818 => Loss: 12.688240\n",
      "Iteration  819 => Loss: 12.667940\n",
      "Iteration  820 => Loss: 12.647840\n",
      "Iteration  821 => Loss: 12.627940\n",
      "Iteration  822 => Loss: 12.608240\n",
      "Iteration  823 => Loss: 12.588740\n",
      "Iteration  824 => Loss: 12.569440\n",
      "Iteration  825 => Loss: 12.550340\n",
      "Iteration  826 => Loss: 12.531440\n",
      "Iteration  827 => Loss: 12.512740\n",
      "Iteration  828 => Loss: 12.494240\n",
      "Iteration  829 => Loss: 12.475940\n",
      "Iteration  830 => Loss: 12.475160\n",
      "Iteration  831 => Loss: 12.454460\n",
      "Iteration  832 => Loss: 12.433960\n",
      "Iteration  833 => Loss: 12.413660\n",
      "Iteration  834 => Loss: 12.393560\n",
      "Iteration  835 => Loss: 12.373660\n",
      "Iteration  836 => Loss: 12.353960\n",
      "Iteration  837 => Loss: 12.334460\n",
      "Iteration  838 => Loss: 12.315160\n",
      "Iteration  839 => Loss: 12.296060\n",
      "Iteration  840 => Loss: 12.277160\n",
      "Iteration  841 => Loss: 12.258460\n",
      "Iteration  842 => Loss: 12.239960\n",
      "Iteration  843 => Loss: 12.221660\n",
      "Iteration  844 => Loss: 12.203560\n",
      "Iteration  845 => Loss: 12.185660\n",
      "Iteration  846 => Loss: 12.167960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  847 => Loss: 12.150460\n",
      "Iteration  848 => Loss: 12.148160\n",
      "Iteration  849 => Loss: 12.128260\n",
      "Iteration  850 => Loss: 12.108560\n",
      "Iteration  851 => Loss: 12.089060\n",
      "Iteration  852 => Loss: 12.069760\n",
      "Iteration  853 => Loss: 12.050660\n",
      "Iteration  854 => Loss: 12.031760\n",
      "Iteration  855 => Loss: 12.013060\n",
      "Iteration  856 => Loss: 11.994560\n",
      "Iteration  857 => Loss: 11.976260\n",
      "Iteration  858 => Loss: 11.958160\n",
      "Iteration  859 => Loss: 11.940260\n",
      "Iteration  860 => Loss: 11.922560\n",
      "Iteration  861 => Loss: 11.905060\n",
      "Iteration  862 => Loss: 11.887760\n",
      "Iteration  863 => Loss: 11.870660\n",
      "Iteration  864 => Loss: 11.853760\n",
      "Iteration  865 => Loss: 11.852540\n",
      "Iteration  866 => Loss: 11.833240\n",
      "Iteration  867 => Loss: 11.814140\n",
      "Iteration  868 => Loss: 11.795240\n",
      "Iteration  869 => Loss: 11.776540\n",
      "Iteration  870 => Loss: 11.758040\n",
      "Iteration  871 => Loss: 11.739740\n",
      "Iteration  872 => Loss: 11.721640\n",
      "Iteration  873 => Loss: 11.703740\n",
      "Iteration  874 => Loss: 11.686040\n",
      "Iteration  875 => Loss: 11.668540\n",
      "Iteration  876 => Loss: 11.651240\n",
      "Iteration  877 => Loss: 11.634140\n",
      "Iteration  878 => Loss: 11.617240\n",
      "Iteration  879 => Loss: 11.600540\n",
      "Iteration  880 => Loss: 11.584040\n",
      "Iteration  881 => Loss: 11.567740\n",
      "Iteration  882 => Loss: 11.567600\n",
      "Iteration  883 => Loss: 11.548900\n",
      "Iteration  884 => Loss: 11.530400\n",
      "Iteration  885 => Loss: 11.512100\n",
      "Iteration  886 => Loss: 11.494000\n",
      "Iteration  887 => Loss: 11.476100\n",
      "Iteration  888 => Loss: 11.458400\n",
      "Iteration  889 => Loss: 11.440900\n",
      "Iteration  890 => Loss: 11.423600\n",
      "Iteration  891 => Loss: 11.406500\n",
      "Iteration  892 => Loss: 11.389600\n",
      "Iteration  893 => Loss: 11.372900\n",
      "Iteration  894 => Loss: 11.356400\n",
      "Iteration  895 => Loss: 11.340100\n",
      "Iteration  896 => Loss: 11.324000\n",
      "Iteration  897 => Loss: 11.308100\n",
      "Iteration  898 => Loss: 11.292400\n",
      "Iteration  899 => Loss: 11.276900\n",
      "Iteration  900 => Loss: 11.275240\n",
      "Iteration  901 => Loss: 11.257340\n",
      "Iteration  902 => Loss: 11.239640\n",
      "Iteration  903 => Loss: 11.222140\n",
      "Iteration  904 => Loss: 11.204840\n",
      "Iteration  905 => Loss: 11.187740\n",
      "Iteration  906 => Loss: 11.170840\n",
      "Iteration  907 => Loss: 11.154140\n",
      "Iteration  908 => Loss: 11.137640\n",
      "Iteration  909 => Loss: 11.121340\n",
      "Iteration  910 => Loss: 11.105240\n",
      "Iteration  911 => Loss: 11.089340\n",
      "Iteration  912 => Loss: 11.073640\n",
      "Iteration  913 => Loss: 11.058140\n",
      "Iteration  914 => Loss: 11.042840\n",
      "Iteration  915 => Loss: 11.027740\n",
      "Iteration  916 => Loss: 11.012840\n",
      "Iteration  917 => Loss: 11.012260\n",
      "Iteration  918 => Loss: 10.994960\n",
      "Iteration  919 => Loss: 10.977860\n",
      "Iteration  920 => Loss: 10.960960\n",
      "Iteration  921 => Loss: 10.944260\n",
      "Iteration  922 => Loss: 10.927760\n",
      "Iteration  923 => Loss: 10.911460\n",
      "Iteration  924 => Loss: 10.895360\n",
      "Iteration  925 => Loss: 10.879460\n",
      "Iteration  926 => Loss: 10.863760\n",
      "Iteration  927 => Loss: 10.848260\n",
      "Iteration  928 => Loss: 10.832960\n",
      "Iteration  929 => Loss: 10.817860\n",
      "Iteration  930 => Loss: 10.802960\n",
      "Iteration  931 => Loss: 10.788260\n",
      "Iteration  932 => Loss: 10.773760\n",
      "Iteration  933 => Loss: 10.759460\n",
      "Iteration  934 => Loss: 10.745360\n",
      "Iteration  935 => Loss: 10.743260\n",
      "Iteration  936 => Loss: 10.726760\n",
      "Iteration  937 => Loss: 10.710460\n",
      "Iteration  938 => Loss: 10.694360\n",
      "Iteration  939 => Loss: 10.678460\n",
      "Iteration  940 => Loss: 10.662760\n",
      "Iteration  941 => Loss: 10.647260\n",
      "Iteration  942 => Loss: 10.631960\n",
      "Iteration  943 => Loss: 10.616860\n",
      "Iteration  944 => Loss: 10.601960\n",
      "Iteration  945 => Loss: 10.587260\n",
      "Iteration  946 => Loss: 10.572760\n",
      "Iteration  947 => Loss: 10.558460\n",
      "Iteration  948 => Loss: 10.544360\n",
      "Iteration  949 => Loss: 10.530460\n",
      "Iteration  950 => Loss: 10.516760\n",
      "Iteration  951 => Loss: 10.503260\n",
      "Iteration  952 => Loss: 10.502240\n",
      "Iteration  953 => Loss: 10.486340\n",
      "Iteration  954 => Loss: 10.470640\n",
      "Iteration  955 => Loss: 10.455140\n",
      "Iteration  956 => Loss: 10.439840\n",
      "Iteration  957 => Loss: 10.424740\n",
      "Iteration  958 => Loss: 10.409840\n",
      "Iteration  959 => Loss: 10.395140\n",
      "Iteration  960 => Loss: 10.380640\n",
      "Iteration  961 => Loss: 10.366340\n",
      "Iteration  962 => Loss: 10.352240\n",
      "Iteration  963 => Loss: 10.338340\n",
      "Iteration  964 => Loss: 10.324640\n",
      "Iteration  965 => Loss: 10.311140\n",
      "Iteration  966 => Loss: 10.297840\n",
      "Iteration  967 => Loss: 10.284740\n",
      "Iteration  968 => Loss: 10.271840\n",
      "Iteration  969 => Loss: 10.259140\n",
      "Iteration  970 => Loss: 10.256600\n",
      "Iteration  971 => Loss: 10.241500\n",
      "Iteration  972 => Loss: 10.226600\n",
      "Iteration  973 => Loss: 10.211900\n",
      "Iteration  974 => Loss: 10.197400\n",
      "Iteration  975 => Loss: 10.183100\n",
      "Iteration  976 => Loss: 10.169000\n",
      "Iteration  977 => Loss: 10.155100\n",
      "Iteration  978 => Loss: 10.141400\n",
      "Iteration  979 => Loss: 10.127900\n",
      "Iteration  980 => Loss: 10.114600\n",
      "Iteration  981 => Loss: 10.101500\n",
      "Iteration  982 => Loss: 10.088600\n",
      "Iteration  983 => Loss: 10.075900\n",
      "Iteration  984 => Loss: 10.063400\n",
      "Iteration  985 => Loss: 10.051100\n",
      "Iteration  986 => Loss: 10.039000\n",
      "Iteration  987 => Loss: 10.037540\n",
      "Iteration  988 => Loss: 10.023040\n",
      "Iteration  989 => Loss: 10.008740\n",
      "Iteration  990 => Loss: 9.994640\n",
      "Iteration  991 => Loss: 9.980740\n",
      "Iteration  992 => Loss: 9.967040\n",
      "Iteration  993 => Loss: 9.953540\n",
      "Iteration  994 => Loss: 9.940240\n",
      "Iteration  995 => Loss: 9.927140\n",
      "Iteration  996 => Loss: 9.914240\n",
      "Iteration  997 => Loss: 9.901540\n",
      "Iteration  998 => Loss: 9.889040\n",
      "Iteration  999 => Loss: 9.876740\n",
      "Iteration 1000 => Loss: 9.864640\n",
      "Iteration 1001 => Loss: 9.852740\n",
      "Iteration 1002 => Loss: 9.841040\n",
      "Iteration 1003 => Loss: 9.829540\n",
      "Iteration 1004 => Loss: 9.829160\n",
      "Iteration 1005 => Loss: 9.815260\n",
      "Iteration 1006 => Loss: 9.801560\n",
      "Iteration 1007 => Loss: 9.788060\n",
      "Iteration 1008 => Loss: 9.774760\n",
      "Iteration 1009 => Loss: 9.761660\n",
      "Iteration 1010 => Loss: 9.748760\n",
      "Iteration 1011 => Loss: 9.736060\n",
      "Iteration 1012 => Loss: 9.723560\n",
      "Iteration 1013 => Loss: 9.711260\n",
      "Iteration 1014 => Loss: 9.699160\n",
      "Iteration 1015 => Loss: 9.687260\n",
      "Iteration 1016 => Loss: 9.675560\n",
      "Iteration 1017 => Loss: 9.664060\n",
      "Iteration 1018 => Loss: 9.652760\n",
      "Iteration 1019 => Loss: 9.641660\n",
      "Iteration 1020 => Loss: 9.630760\n",
      "Iteration 1021 => Loss: 9.620060\n",
      "Iteration 1022 => Loss: 9.618160\n",
      "Iteration 1023 => Loss: 9.605060\n",
      "Iteration 1024 => Loss: 9.592160\n",
      "Iteration 1025 => Loss: 9.579460\n",
      "Iteration 1026 => Loss: 9.566960\n",
      "Iteration 1027 => Loss: 9.554660\n",
      "Iteration 1028 => Loss: 9.542560\n",
      "Iteration 1029 => Loss: 9.530660\n",
      "Iteration 1030 => Loss: 9.518960\n",
      "Iteration 1031 => Loss: 9.507460\n",
      "Iteration 1032 => Loss: 9.496160\n",
      "Iteration 1033 => Loss: 9.485060\n",
      "Iteration 1034 => Loss: 9.474160\n",
      "Iteration 1035 => Loss: 9.463460\n",
      "Iteration 1036 => Loss: 9.452960\n",
      "Iteration 1037 => Loss: 9.442660\n",
      "Iteration 1038 => Loss: 9.432560\n",
      "Iteration 1039 => Loss: 9.431740\n",
      "Iteration 1040 => Loss: 9.419240\n",
      "Iteration 1041 => Loss: 9.406940\n",
      "Iteration 1042 => Loss: 9.394840\n",
      "Iteration 1043 => Loss: 9.382940\n",
      "Iteration 1044 => Loss: 9.371240\n",
      "Iteration 1045 => Loss: 9.359740\n",
      "Iteration 1046 => Loss: 9.348440\n",
      "Iteration 1047 => Loss: 9.337340\n",
      "Iteration 1048 => Loss: 9.326440\n",
      "Iteration 1049 => Loss: 9.315740\n",
      "Iteration 1050 => Loss: 9.305240\n",
      "Iteration 1051 => Loss: 9.294940\n",
      "Iteration 1052 => Loss: 9.284840\n",
      "Iteration 1053 => Loss: 9.274940\n",
      "Iteration 1054 => Loss: 9.265240\n",
      "Iteration 1055 => Loss: 9.255740\n",
      "Iteration 1056 => Loss: 9.246440\n",
      "Iteration 1057 => Loss: 9.244100\n",
      "Iteration 1058 => Loss: 9.232400\n",
      "Iteration 1059 => Loss: 9.220900\n",
      "Iteration 1060 => Loss: 9.209600\n",
      "Iteration 1061 => Loss: 9.198500\n",
      "Iteration 1062 => Loss: 9.187600\n",
      "Iteration 1063 => Loss: 9.176900\n",
      "Iteration 1064 => Loss: 9.166400\n",
      "Iteration 1065 => Loss: 9.156100\n",
      "Iteration 1066 => Loss: 9.146000\n",
      "Iteration 1067 => Loss: 9.136100\n",
      "Iteration 1068 => Loss: 9.126400\n",
      "Iteration 1069 => Loss: 9.116900\n",
      "Iteration 1070 => Loss: 9.107600\n",
      "Iteration 1071 => Loss: 9.098500\n",
      "Iteration 1072 => Loss: 9.089600\n",
      "Iteration 1073 => Loss: 9.080900\n",
      "Iteration 1074 => Loss: 9.079640\n",
      "Iteration 1075 => Loss: 9.068540\n",
      "Iteration 1076 => Loss: 9.057640\n",
      "Iteration 1077 => Loss: 9.046940\n",
      "Iteration 1078 => Loss: 9.036440\n",
      "Iteration 1079 => Loss: 9.026140\n",
      "Iteration 1080 => Loss: 9.016040\n",
      "Iteration 1081 => Loss: 9.006140\n",
      "Iteration 1082 => Loss: 8.996440\n",
      "Iteration 1083 => Loss: 8.986940\n",
      "Iteration 1084 => Loss: 8.977640\n",
      "Iteration 1085 => Loss: 8.968540\n",
      "Iteration 1086 => Loss: 8.959640\n",
      "Iteration 1087 => Loss: 8.950940\n",
      "Iteration 1088 => Loss: 8.942440\n",
      "Iteration 1089 => Loss: 8.934140\n",
      "Iteration 1090 => Loss: 8.926040\n",
      "Iteration 1091 => Loss: 8.925860\n",
      "Iteration 1092 => Loss: 8.915360\n",
      "Iteration 1093 => Loss: 8.905060\n",
      "Iteration 1094 => Loss: 8.894960\n",
      "Iteration 1095 => Loss: 8.885060\n",
      "Iteration 1096 => Loss: 8.875360\n",
      "Iteration 1097 => Loss: 8.865860\n",
      "Iteration 1098 => Loss: 8.856560\n",
      "Iteration 1099 => Loss: 8.847460\n",
      "Iteration 1100 => Loss: 8.838560\n",
      "Iteration 1101 => Loss: 8.829860\n",
      "Iteration 1102 => Loss: 8.821360\n",
      "Iteration 1103 => Loss: 8.813060\n",
      "Iteration 1104 => Loss: 8.804960\n",
      "Iteration 1105 => Loss: 8.797060\n",
      "Iteration 1106 => Loss: 8.789360\n",
      "Iteration 1107 => Loss: 8.781860\n",
      "Iteration 1108 => Loss: 8.774560\n",
      "Iteration 1109 => Loss: 8.772860\n",
      "Iteration 1110 => Loss: 8.763160\n",
      "Iteration 1111 => Loss: 8.753660\n",
      "Iteration 1112 => Loss: 8.744360\n",
      "Iteration 1113 => Loss: 8.735260\n",
      "Iteration 1114 => Loss: 8.726360\n",
      "Iteration 1115 => Loss: 8.717660\n",
      "Iteration 1116 => Loss: 8.709160\n",
      "Iteration 1117 => Loss: 8.700860\n",
      "Iteration 1118 => Loss: 8.692760\n",
      "Iteration 1119 => Loss: 8.684860\n",
      "Iteration 1120 => Loss: 8.677160\n",
      "Iteration 1121 => Loss: 8.669660\n",
      "Iteration 1122 => Loss: 8.662360\n",
      "Iteration 1123 => Loss: 8.655260\n",
      "Iteration 1124 => Loss: 8.648360\n",
      "Iteration 1125 => Loss: 8.641660\n",
      "Iteration 1126 => Loss: 8.641040\n",
      "Iteration 1127 => Loss: 8.631940\n",
      "Iteration 1128 => Loss: 8.623040\n",
      "Iteration 1129 => Loss: 8.614340\n",
      "Iteration 1130 => Loss: 8.605840\n",
      "Iteration 1131 => Loss: 8.597540\n",
      "Iteration 1132 => Loss: 8.589440\n",
      "Iteration 1133 => Loss: 8.581540\n",
      "Iteration 1134 => Loss: 8.573840\n",
      "Iteration 1135 => Loss: 8.566340\n",
      "Iteration 1136 => Loss: 8.559040\n",
      "Iteration 1137 => Loss: 8.551940\n",
      "Iteration 1138 => Loss: 8.545040\n",
      "Iteration 1139 => Loss: 8.538340\n",
      "Iteration 1140 => Loss: 8.531840\n",
      "Iteration 1141 => Loss: 8.525540\n",
      "Iteration 1142 => Loss: 8.519440\n",
      "Iteration 1143 => Loss: 8.513540\n",
      "Iteration 1144 => Loss: 8.511400\n",
      "Iteration 1145 => Loss: 8.503100\n",
      "Iteration 1146 => Loss: 8.495000\n",
      "Iteration 1147 => Loss: 8.487100\n",
      "Iteration 1148 => Loss: 8.479400\n",
      "Iteration 1149 => Loss: 8.471900\n",
      "Iteration 1150 => Loss: 8.464600\n",
      "Iteration 1151 => Loss: 8.457500\n",
      "Iteration 1152 => Loss: 8.450600\n",
      "Iteration 1153 => Loss: 8.443900\n",
      "Iteration 1154 => Loss: 8.437400\n",
      "Iteration 1155 => Loss: 8.431100\n",
      "Iteration 1156 => Loss: 8.425000\n",
      "Iteration 1157 => Loss: 8.419100\n",
      "Iteration 1158 => Loss: 8.413400\n",
      "Iteration 1159 => Loss: 8.407900\n",
      "Iteration 1160 => Loss: 8.402600\n",
      "Iteration 1161 => Loss: 8.401540\n",
      "Iteration 1162 => Loss: 8.393840\n",
      "Iteration 1163 => Loss: 8.386340\n",
      "Iteration 1164 => Loss: 8.379040\n",
      "Iteration 1165 => Loss: 8.371940\n",
      "Iteration 1166 => Loss: 8.365040\n",
      "Iteration 1167 => Loss: 8.358340\n",
      "Iteration 1168 => Loss: 8.351840\n",
      "Iteration 1169 => Loss: 8.345540\n",
      "Iteration 1170 => Loss: 8.339440\n",
      "Iteration 1171 => Loss: 8.333540\n",
      "Iteration 1172 => Loss: 8.327840\n",
      "Iteration 1173 => Loss: 8.322340\n",
      "Iteration 1174 => Loss: 8.317040\n",
      "Iteration 1175 => Loss: 8.311940\n",
      "Iteration 1176 => Loss: 8.307040\n",
      "Iteration 1177 => Loss: 8.302340\n",
      "Iteration 1178 => Loss: 8.297840\n",
      "Iteration 1179 => Loss: 8.295260\n",
      "Iteration 1180 => Loss: 8.288360\n",
      "Iteration 1181 => Loss: 8.281660\n",
      "Iteration 1182 => Loss: 8.275160\n",
      "Iteration 1183 => Loss: 8.268860\n",
      "Iteration 1184 => Loss: 8.262760\n",
      "Iteration 1185 => Loss: 8.256860\n",
      "Iteration 1186 => Loss: 8.251160\n",
      "Iteration 1187 => Loss: 8.245660\n",
      "Iteration 1188 => Loss: 8.240360\n",
      "Iteration 1189 => Loss: 8.235260\n",
      "Iteration 1190 => Loss: 8.230360\n",
      "Iteration 1191 => Loss: 8.225660\n",
      "Iteration 1192 => Loss: 8.221160\n",
      "Iteration 1193 => Loss: 8.216860\n",
      "Iteration 1194 => Loss: 8.212760\n",
      "Iteration 1195 => Loss: 8.208860\n",
      "Iteration 1196 => Loss: 8.207360\n",
      "Iteration 1197 => Loss: 8.201060\n",
      "Iteration 1198 => Loss: 8.194960\n",
      "Iteration 1199 => Loss: 8.189060\n",
      "Iteration 1200 => Loss: 8.183360\n",
      "Iteration 1201 => Loss: 8.177860\n",
      "Iteration 1202 => Loss: 8.172560\n",
      "Iteration 1203 => Loss: 8.167460\n",
      "Iteration 1204 => Loss: 8.162560\n",
      "Iteration 1205 => Loss: 8.157860\n",
      "Iteration 1206 => Loss: 8.153360\n",
      "Iteration 1207 => Loss: 8.149060\n",
      "Iteration 1208 => Loss: 8.144960\n",
      "Iteration 1209 => Loss: 8.141060\n",
      "Iteration 1210 => Loss: 8.137360\n",
      "Iteration 1211 => Loss: 8.133860\n",
      "Iteration 1212 => Loss: 8.130560\n",
      "Iteration 1213 => Loss: 8.130140\n",
      "Iteration 1214 => Loss: 8.124440\n",
      "Iteration 1215 => Loss: 8.118940\n",
      "Iteration 1216 => Loss: 8.113640\n",
      "Iteration 1217 => Loss: 8.108540\n",
      "Iteration 1218 => Loss: 8.103640\n",
      "Iteration 1219 => Loss: 8.098940\n",
      "Iteration 1220 => Loss: 8.094440\n",
      "Iteration 1221 => Loss: 8.090140\n",
      "Iteration 1222 => Loss: 8.086040\n",
      "Iteration 1223 => Loss: 8.082140\n",
      "Iteration 1224 => Loss: 8.078440\n",
      "Iteration 1225 => Loss: 8.074940\n",
      "Iteration 1226 => Loss: 8.071640\n",
      "Iteration 1227 => Loss: 8.068540\n",
      "Iteration 1228 => Loss: 8.065640\n",
      "Iteration 1229 => Loss: 8.062940\n",
      "Iteration 1230 => Loss: 8.060440\n",
      "Iteration 1231 => Loss: 8.058500\n",
      "Iteration 1232 => Loss: 8.053600\n",
      "Iteration 1233 => Loss: 8.048900\n",
      "Iteration 1234 => Loss: 8.044400\n",
      "Iteration 1235 => Loss: 8.040100\n",
      "Iteration 1236 => Loss: 8.036000\n",
      "Iteration 1237 => Loss: 8.032100\n",
      "Iteration 1238 => Loss: 8.028400\n",
      "Iteration 1239 => Loss: 8.024900\n",
      "Iteration 1240 => Loss: 8.021600\n",
      "Iteration 1241 => Loss: 8.018500\n",
      "Iteration 1242 => Loss: 8.015600\n",
      "Iteration 1243 => Loss: 8.012900\n",
      "Iteration 1244 => Loss: 8.010400\n",
      "Iteration 1245 => Loss: 8.008100\n",
      "Iteration 1246 => Loss: 8.006000\n",
      "Iteration 1247 => Loss: 8.004100\n",
      "Iteration 1248 => Loss: 8.003240\n",
      "Iteration 1249 => Loss: 7.998940\n",
      "Iteration 1250 => Loss: 7.994840\n",
      "Iteration 1251 => Loss: 7.990940\n",
      "Iteration 1252 => Loss: 7.987240\n",
      "Iteration 1253 => Loss: 7.983740\n",
      "Iteration 1254 => Loss: 7.980440\n",
      "Iteration 1255 => Loss: 7.977340\n",
      "Iteration 1256 => Loss: 7.974440\n",
      "Iteration 1257 => Loss: 7.971740\n",
      "Iteration 1258 => Loss: 7.969240\n",
      "Iteration 1259 => Loss: 7.966940\n",
      "Iteration 1260 => Loss: 7.964840\n",
      "Iteration 1261 => Loss: 7.962940\n",
      "Iteration 1262 => Loss: 7.961240\n",
      "Iteration 1263 => Loss: 7.959740\n",
      "Iteration 1264 => Loss: 7.958440\n",
      "Iteration 1265 => Loss: 7.957340\n",
      "Iteration 1266 => Loss: 7.954960\n",
      "Iteration 1267 => Loss: 7.951460\n",
      "Iteration 1268 => Loss: 7.948160\n",
      "Iteration 1269 => Loss: 7.945060\n",
      "Iteration 1270 => Loss: 7.942160\n",
      "Iteration 1271 => Loss: 7.939460\n",
      "Iteration 1272 => Loss: 7.936960\n",
      "Iteration 1273 => Loss: 7.934660\n",
      "Iteration 1274 => Loss: 7.932560\n",
      "Iteration 1275 => Loss: 7.930660\n",
      "Iteration 1276 => Loss: 7.928960\n",
      "Iteration 1277 => Loss: 7.927460\n",
      "Iteration 1278 => Loss: 7.926160\n",
      "Iteration 1279 => Loss: 7.925060\n",
      "Iteration 1280 => Loss: 7.924160\n",
      "Iteration 1281 => Loss: 7.923460\n",
      "Iteration 1282 => Loss: 7.922960\n",
      "Iteration 1283 => Loss: 7.921660\n",
      "Iteration 1284 => Loss: 7.918760\n",
      "Iteration 1285 => Loss: 7.916060\n",
      "Iteration 1286 => Loss: 7.913560\n",
      "Iteration 1287 => Loss: 7.911260\n",
      "Iteration 1288 => Loss: 7.909160\n",
      "Iteration 1289 => Loss: 7.907260\n",
      "Iteration 1290 => Loss: 7.905560\n",
      "Iteration 1291 => Loss: 7.904060\n",
      "Iteration 1292 => Loss: 7.902760\n",
      "Iteration 1293 => Loss: 7.901660\n",
      "Iteration 1294 => Loss: 7.900760\n",
      "Iteration 1295 => Loss: 7.900060\n",
      "Iteration 1296 => Loss: 7.899560\n",
      "Iteration 1297 => Loss: 7.899260\n",
      "Iteration 1298 => Loss: 7.899160\n",
      "\n",
      "w=1.670, b=10.090\n",
      "Prediction: x=20 => y=43.49\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Import data\n",
    "    X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)\n",
    "    # Train system\n",
    "    w, b = train(X, Y, iterations=10000, lr=0.01)\n",
    "    print(\"\\nw=%.3f, b=%.3f\" % (w, b))\n",
    "    print(\"Prediction: x=%d => y=%.2f\" % (20, predict(20, w, b)))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set() # activate Seaborn\n",
    "plt.axis([0, 50, 0, 50]) # scale axes (0 to 50)\n",
    "plt.xlabel(\"Reservations\", fontsize=20) # set x axis label\n",
    "plt.ylabel(\"Pizzas\", fontsize=20) # set y axis label\n",
    "#X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True) # load data\n",
    "ax = sns.regplot(x=X, y=Y, color=\"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
